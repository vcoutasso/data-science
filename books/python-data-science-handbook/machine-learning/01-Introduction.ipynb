{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Machine learning is where computational and algorithmic skills of data science meet the statistical thinking of data science, resulting in a collection of approaches to inference and data exploration.\n",
    "\n",
    "While machine methods can be incredibly powerful, they are most definitely not a panacea. To be effective, they must be approached with a firm grasp of the strengths and weaknesses of each method, as well as a  grasp of general concepts such as bias and variance, overfitting and underfitting, and more.\n",
    "\n",
    "This chapter aims to dive into practical aspects of machine learning, using Python's [Scikit-Learn](https://scikit-learn.org/) package as the primary tool.\n",
    "\n",
    "The goals of this chapter are:\n",
    "\n",
    "- Introduce the fundamental vocabulary and concepts of machine learning.\n",
    "- Introduce the Scikit-Learn API.\n",
    "- Take a deeper dive into the details of several of the most important machine learning approaches, and develop an intuition into how they work and when and where they are applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "\n",
    "Fundamentally, machine learning involves building mathematical models to help understand data. The \"learning\" bit comes from the fact the these models have _tunable parameters_ that can be adapted to observed data; in this way the program can be considered to be \"learning\" from the data. Once these models have been fit to previously seen data, they can be used to predict and understand aspects of newly observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories of Machine Learning\n",
    "\n",
    "Machine learning can be categorized into two main types: supervised learning and unsupervised learning.\n",
    "\n",
    "_Supervised learning_ involves modeling the relationship between measured features of data and some label (known _a priori_) associated with it; once this model is determined, it can be used to apply labels to new, unknown data. This is further subdivided into _classification_ and _regressions_ tasks: in classification, the labels are discrete categories, while in regression, the labels are continuous quantities.\n",
    "\n",
    "_Unsupervised learning_ involves modeling the features of a dataset without reference to any label, and can be thought of as \"letting the dataset speak for itself\". Tasks such as _clustering_ and _dimensionality reduction_ fall into this category. Clustering algorithms identify distinct groups of data, while dimensionality reduction algorithms can present a more succinct representation of the data. \n",
    "\n",
    "In addition, somewhere in between supervised learning and unsupervised learning, _semi-supervised_ methods are found. These are generally useful when only incomplete labels are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Examples of Machine Learning Applications\n",
    "\n",
    "To make these ideas more concrete, we will take a look at a few simples examples of machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification: Predicting discrete labels\n",
    "\n",
    "In a classification task, we have a set of labeled points and need to use these to classify some unlabeled points. Consider the data shown in the following figure:\n",
    "\n",
    "![Classification data](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-1.png)\n",
    "\n",
    "Here we have a two-dimensional dataset: each data point has two _features_, represented by the $(x, y)$ positions on the plane. In addition, there is an extra piece of information we have about this data: one of two _class labels_ for each point, represented by the colors of the points on the image above. From these features and labels, we would like to create a model that is able to decide whether a new point should be labeled \"blue\" or \"red\".\n",
    "\n",
    "There are many possible models for such a classification task, but we will use a simple one. We will work under the assumption that the two groups can be separated by drawing a straight line through the plane between them. The optimal value for the model parameters (which would describe the location and orientation of the line) are learned from the data, which is often called _training the model_.\n",
    "\n",
    "The following figure shows a visualization of what such model could look like:\n",
    "\n",
    "![Trained linear classifier](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-2.png)\n",
    "\n",
    "Now that the model is trained, it can be generalized to new, unlabeled data. This means that we can take a new set of data, draw this line that the model learned through it, and assign labels to the points based on this model. This stage is usually called _prediction_:\n",
    "\n",
    "![Prediction of new data](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-classification-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression: Predicting continuous labels\n",
    "\n",
    "In contrast with the discrete labels of a classification task, in a regression task the labels are continuous quantities.\n",
    "\n",
    "Consider the data shown in the following figure, which consists of a set of points each with a continuous label:\n",
    "\n",
    "![Regression data](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-1.png)\n",
    "\n",
    "Once again, we are dealing with two-dimensional data and, since it is a supervised task, we also have the labels for each data point. There are a number of possible regression models we could use for this type of data, but we will use a simple linear regression to predict the points.\n",
    "\n",
    "We can visualize this data in a three-dimensional space where the third dimensional is the value of the continuous label:\n",
    "\n",
    "![3D visualization](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-2.png)\n",
    "\n",
    "From this view, it seems reasonable that fitting a plane through this three-dimensional data would allow up to predict the expected label. When we fit such a plane we get the following result:\n",
    "\n",
    "![Regression result](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-3.png)\n",
    "\n",
    "With this fitted plane, we have all we need to predict labels for new points:\n",
    "\n",
    "![Regression prediction](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-regression-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering: Inferring labels on unlabeled data\n",
    "\n",
    "Clustering is a very common case of unsupervised learning in which data is assigned to some number of discrete groups. For example, we might have some two-dimensional data like that shown in the following figure:\n",
    "\n",
    "![Clustering data](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-clustering-1.png)\n",
    "\n",
    "Judging by eye, it is pretty easy (for a human) to observe maybe 3 or 4 distinct groups of data. A clustering model uses the intrinsic structure of the data to determine which points are related in order to draw a similar conclusion. Using the k-means clustering algorithm with a value of $k = 4$, we can find the following clusters:\n",
    "\n",
    "![k-means result](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-clustering-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction: Inferring structure of unlabeled data\n",
    "\n",
    "Dimensionality reduction is another example of an unsupervised learning task. It generally seeks to pull out some low-dimensional representation of data that preserves relevant qualities of the full dataset. \n",
    "\n",
    "As an example, consider the following data:\n",
    "\n",
    "![Dimensionality reduction data](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-dimesionality-1.png)\n",
    "\n",
    "We can see that there is some structure in this data: it is drawn from a one-dimensional line that is arranged in a spiral within this two-dimensional space. We could say that this data is \"intrinsically\" only one-dimensional, despite being embedded in a higher-dimensional space. A suitable dimensionality reduction model would be sensitive to this nonlinear embedded structure, and be able to pull out this lower-dimensionality representation.\n",
    "\n",
    "The following figure shows a visualization of the results of the Isomap algorithm, a manifold learning algorithm that does exactly this:\n",
    "\n",
    "![Isomap result](https://jakevdp.github.io/PythonDataScienceHandbook/figures/05.01-dimesionality-2.png)\n",
    "\n",
    "Notice that the colors (which represent the extracted one-dimensional latent variable) change uniformly along the spiral, which indicates that the algorithm detected the structure we saw by eye. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:handbook]",
   "language": "python",
   "name": "conda-env-handbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
