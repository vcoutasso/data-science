#+TITLE: Hands-On Machine Learning Notes

* Table of contents :TOC_2:
- [[#part-i---the-fundamentals-of-machine-learning][Part I - The Fundamentals of Machine Learning]]
  - [[#01-the-machine-learning-landscape][01. The Machine Learning Landscape]]
  - [[#02-end-to-end-machine-learning-project][02. End-to-End Machine Learning Project]]
  - [[#03-classification][03. Classification]]
  - [[#04-training-models][04. Training Models]]

* Part I - The Fundamentals of Machine Learning

** 01. The Machine Learning Landscape

*** What is Machine Learning?

Machine Learning is the science and art behind designing computer systems that can learn from data.

#+BEGIN_QUOTE
A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $P$, improves with experience $E$. -- Tom Mitchell, 1997
#+END_QUOTE

Applying ML techniques to data can help discovert patters that were not immediately apparent. This is called /data mining/, and is part of a process known as Knowledge Discovery in Databases (KDD).

*** Types of Machine Learning Systems

ML systems can be classified in broad categories, based on the following criteria:

- Whether or not human supervision is part of the training process (supervised, unsupervised, semi-supervised and reinforcement learning).
- If the system can learn incrementally on the fly (online learning) or not (batch learning).
- Works by comparing new data points to known data points (instance-based learning) or by detecting patterns in the training phase and building a predictive model (model-based learning).

**** Supervised learning

- Classification (classifies/labels new instances)
- Regression (predicts a target value for new instances)

**** Unsupervised learning

- Clustering (detects groups of similar data points)
- Anomaly/novelty detection (detects anomalies (outliers)/novelty classes)
- Data visualization and dimensionality reduction (creates lower-dimensional representations of the data)
- Association rule learning (discovers relations between attributes)

**** Semi-supervised learning

These methods (most of them combinations of supervised and unsupervised algorithms) can deal with partially labeled datasets (e.g. plenty of unlabeled instances, and few labeled instances).

**** Reinforcement learning

Trains an /agent/ which can perform actions and get /rewards/ (or /penalties/) in return. It learns by itself what is the best strategy (/policy/) to get the most reward over time.

**** Batch learning

The system is not capable of learning incrementally, thus it must be trained using all the available data first, so it can be launched into production (where it runs without learning anymore). This is called /offline learning/.
For a batch learning system to know about new data, a new version of it must be trained from scratch on the full (now augmented) dataset.

**** Online learning

As opposed to batch learning, these system are trained incrementally by feeding it data instances sequentially in groups called /mini-batches/. This strategy is most suitable for systems that need to adapt to change rapidly or autonomously. This idea can also be used to train systems on datasets too large to fit in memory (/out-of-core/ learning).

**** Instance-based learning

This form of learning can be thought of as simply learning by heart: the system learns the training instances by heart, then generalizes to new cases using a similarity measure to compare them to the learned examples.

**** Model-based learning

Another way to generalize from a training set is to build a model of these data points and then use that model to make /predictions/ on new data (this is called /inference/).

*** Challenges of Machine Learning

**** Insufficient Quantity of Training Data

This [[https://dl.acm.org/doi/10.3115/1073012.1073017][famous paper]] from 2001 showed that very different Machine Learning algorithms performed almost identically on a complex problem, once they were given enough data. Unfortunately, it is not always feasible to obtain extra data.

**** Non-representative Training Data

For a model to be successful, it is crucial that the data it was trained on is representative of the new cases it must generalize to. Some problems we might come across are: if the sample is too small, we can have /sampling noise/ (the data is non-representative as a result of chance); if the sampling method is flawed, even large samples can be non-representative. This is called /sampling bias/.

**** Poor-Quality Data

Preparing and cleaning up datasets takes a significant part of a data scientist's time. This is very important because if the training data is full of problems, it will make it much harder for the system to detect the underlying patterns.

**** Irrelevant Features

The process of coming up with a good set of features to train a model on is called /feature engineering/, and involves the following steps:

- /Feature selection/, which consists of selecting the most useful features for training.
- /Feature extraction/, which consists of combining existing features to produce more useful ones (dimensionality reduction can help with that).
- Creating new features by gathering new data.

**** Overfitting the Training Data

We can identify overfitting when a model performs well on the training data, but it does not generalize well for new instances. Complex models can detect subtle patterns in the data, but if the training set is too noisy or too small, the model is likely to detect patterns in the noise itself.

We can reduce the risk of overfitting by constraining a model, that is, applying /regularization/ to it. There are many different kinds regularization strategies, but they essentially seek to constrain the degree of freedom of a given model, basically ensuring simpler models.

The amount of regularization to apply during learning is controlled by a /hyperparameter/ that must be set prior to training and remains constant.

Options to fix this:

- Simplify the model (use fewer parameters, reduce the number of attributes in the data, or constrain the model).
- Use more training data.
- Reduce the noise in the data (fix data issues and remove outliers).

**** Underfitting the Training Data

Underfitting is the opposite of overfitting: if a model is too simple to learn the underlying structure of the data, its performance will be greatly reduced.

Options to fix this:

- Use a more powerful/complex model.
- Use better features (feature engineering).
- Reduce the constraints (regularization hyperparameter).

*** Testing and Validating

Split the data into two sets: the /training set/ and the /testing set/. It's common to use 80% of the data for training and /hold out/ 20% for testing.

The error rate on new cases is called the /generalization error/ (or /out-of-sample error/). By evaluating a model on the test set, we can get an estimate of this error.

**** Hyperparameter Tuning and Model Selection

If we evaluate the model on the test set too many times and use the results to "improve" it, we might be overfitting the model to the test data.

A common solution to this problem is the /holdout validation/: we hold out part of the training set (this new subset is called /validation set/) to evaluate candidate models and select the best one. After this validation process, we train the model on the full training set (including the validation set) to get the final model, which is then evaluated on the test set.

This works quite well, but can lead to problems: for a validation set too small we take our chances with sampling bias, since we can end up with a non-representative subset of the data. If the validation set is too large, the remaining training set will be much smaller than the full training set.

This issues can be solved by performing repeated /cross-validation/: we split the training set in $k$ subsets of equal size, and use each in turn as a validation set. We then average out all the evaluations of a model, resulting in a much more accurate measure of its performance. The drawback here is that the training time is multiplied by the number of validation sets $k$.

**** Data Mismatch

It might be the case in some applications that the data the model was trained on does not represent the data the model will work with in production. To avoid this, the validation and the test set must be as representative as possible of the data the model will use in production.

If there is a risk of mismatch, we can use yet another set (by holding out some of the training set) that Andrew Ng calls the /train-dev set/. The model is trained on the rest of the training set, and then evaluated on both the train-dev set and the validation set. If the model performs well on the training set, but not on the train-dev set, it is likely overfitting the training data.  If it performs well on both the training set and the train-dev set, but not on the validation set, there is probably some mismatch between the training data and the validation + test data.

** 02. End-to-End Machine Learning Project

*** Look at the Big Picture

**** Some terminology

- /Multiple regression/ problem: a problem in which the system uses multiple features to make a prediction.
- /Univariate regression/ problem: a problem in which we are only trying to predict a single value.
- /Multivariate regression/ problem: a problem in which we are trying to predict multiple values.
- /Hypothesis/: a machine learning system's prediction function may be called a hypothesis, usually denoted by $h$.

**** Performance Measure

- Root-mean-square deviation (RMSE) is generally the preferred performance measure for regressions tasks, although very sensible to outliers. It corresponds to the _Euclidean norm_, also called the $\ell_2$ /norm/,
   noted $\|\cdot\|_2$ (or just $\|\cdot\|$).
- Mean absolute error (MAE), also called the average absolute deviation, is a good option in the presence of outliers. It corresponds to the $\ell_1$ /norm/, noted $\|\cdot\|_1$. This is sometimes called the /Manhattan norm/.
- A /norm/ is a distance measure. The $\ell_k$ /norm/ of a vector $\bold{v}$ containing $n$ elements is defined as $\|\bold{v}\|_k = \left( |v_0|^k + |v_1|^k + \dots + |v_n|^k \right)^{\frac{1}{k}}$. $\ell_0$ gives the number of nonzero elements in the vector. $\ell_\infty$ gives the maximum absolute value in the vector.
- The higher the norm index $k$, the more if focuses on large values in detriment of smaller ones. That why RMSE ($\ell_2$) is more sensitive to outliers than MAE ($\ell_1$). However, if outliers are exponentially rare, RMSE still performs very well and is generally preferred.

*** Get the Data

A /tail-heavy/ histogram extends much farther to the right of the median than to the left. Feature distributions such as this may make it harder for some ML algorithms to detect patterns. When possible, consider transforming these features (by computing their logarithm, for example).

**** Check the Assumptions!

It's good practice to thoroughly list and verify the assumptions made about the problem at hand. This can help catch serious issues early on, possibly preventing some gigantic headaches.

**** Test Set

This subject is extremely delicate and incorrect handling of test data may lead to creating (and worse: deploying) biased models. Some common mistakes to be aware of:

- Estimating the generalization error using the test set may lead to very optimistic (and quite possibly unrealistic) estimates. This is called /data snooping/ bias.
- The train/test split should be stable. If in every training iteration the data is split again, the model may get to see the whole dataset over time, which we want to avoid.
- Purely random sampling methods are generally fine if the dataset is large and balanced enough. If not, we run the risk of introducing a significant sampling bias.
- /Stratified sampling/ solves the issue of introducing sampling bias: the data is divided into homogeneous subgroups called /strata/, and the data is sampled in such a way that each stratum is guaranteed to be representative of the overall population. Notice that, if there is not a sufficient number of instances for each stratum, the estimate of a stratum's importance may be biased.

*** Discover and Visualize the Data

The ~jet~ color map ranges from blue to red, and it is great for visualizing density, for example.

**** Correlations

- The /standard correlation coefficient/ (also called /Pearson's r/) can be computed between every pair of attributes to discover linear correlation between them. This coefficient ranges from -1 to 1. When close to 1, it indicates strong positive correlation. When close to -1, it indicates strong negative correlation. Coefficients close to 0 mean that there is no linear correlation.
- Pandas' ~scatter_matrix()~ plots every numerical attribute against every other numerical attribute. The number of plots grows quadratically, so it might be a good idea to focus only on a few promising attributes depending on the dataset.

*** Prepare the Data

Using functions to prepare the data for ML algorithms is good practice. This allows for ease of reproduction, the habit may lead to a neat little library of common transformation functions, and the modularity allows for lots of flexibility when trying out different combinations of transformations.

**** Data Cleaning

Real world data rarely comes tidy and ready to be fed to ML algorithms: datasets often are filled with missing values among other problems. When dealing with missing values, we have three options:

1. Get rid of every sample that contain missing data.
2. Get rid of the whole attribute.
3. Set these values to some pre-determined value (e.g. zero, the mean, the median).

When working with the option 3, the median value (for example) should be computed using the training set to fill it. It's important to save these values for later use: they will be need to replace the missing values in the test set, as well as on new data when the system goes live. Scikit-Learn provides a handy class to take cara of missing values: ~SingleImputer~.


**** Text and Categorical Attributes

It's quite common for categorical attributes to be represented as text (e.g. low, normal, high). Most ML algorithms prefer to work with numbers, so we can convert these categories from text to numbers. Scikit-Learn's ~OrdinalEncoder~ is a great tool for just that!

One issue with that is that the algorithms will assume that two nearby numerical values are more similar than two distant values. We can avoid this by using what's called /one-hot encoding/, adding extra binary attributes that represent the categorical values. This is called /one-hot encoding/, and Scikit-Learn provides the ~OneHotEncoder~ class to do this.

**** Feature Scaling

ML algorithms generally don't perform well when the input attributes have very different scales. We have two main approaches to address this issue: /min-max scaling/ (also called /normalization/) and /standardization/:

- Normalization rescales the values so that they end up ranging from 0 to 1 (or some other arbitrary range).
- Standardization first subtracts the mean value, then it divides by the standard deviation. It does not bound values to some pre-determined range, but it's much less affected by outliers.

Important: the scalers should be fed *the training data only* to prevent any kind of bias.

*** Selecting and Training a Model

/K-fold cross-validation/ is usually a good strategy for a reliable evaluation of a model.

It is good practice to save models we experiment with. The /pickle/ module lets us do just that, serializing the model and saving it as a file. The /joblib/ library is another option, which is more efficient at serializing large NumPy arrays.

*** Fine-Tuning a Model

**** Grid Search and Randomized Search

~GridSearchCV~ is a neat little tool that searches for the best combination of hyperarameters for us, given a set of values to be tested. It uses cross-validation to evaluate all the possible combinations. One thing to keep in mind is that if the best value for a given hyperparameter is the largest value of the range of possibilites supplied, it might be a good idea to search again with higher values (we might find something even better!).

The problem with the grid search approach is that it's very computationally expensive: a model is trained once for every single combination of hyperparameters. With a model complex enough and many combinations to test, the task can grow to become intractable in reasonable time pretty quickly. For occasions like this, it is often preferable to use ~RandomizedSearchCV~ instead. It evaluates a given number of random combinations, instead of all of them. With this we have much more control of how much time we spend.

**** Analyze the Best Models

We will often gain good insights on the problem by inspecting the best models. For example, the ~RandomForestRegressor~ estimator can indicate the relative of each attribute for making predictions!

**** Evaluate the System on the Test Set

This is the final step of creating a model, and the only moment we really deal with the test set.

Tip: If we want to have an idea of how precise and estimate is, we can compute a 95% /confidence interval/ for the generalization error using ~scipy.stats.t.interval()~.

*** Launching, Monitoring and Maintaining a System

The fact is, we need to monitor a model's live performance. Relevant processes may fail (we need to be prepared for dealing with those), performance may degrade because of a poor-quality input signal (we could monitor inputs somehow to detect these), and data that keeps evolving may render a model useless over time.

It's important to keep backups of every model used, as well as the tools to properly and quickly work with them.

** 03. Classification

*** MNIST

The MNIST dataset is a set of 70,000 small images of handwritten digits. This set has been studied so much that it is often called the "hello world" of Machine Learning. Each image is 28 $\times$ 28 pixels (totaling 784), and this dataset is already split in a training set (the first 60,000 images) and a test set (the last 10,000 images).

Some ML algorithms are sensitive to the order of training instances, so feeding them many similar instances in a row might affect performance. Shuffling the dataset beforehand is a good idea since it ensures that this won't happen.

*** Training a Binary Classifier

A /binary classifier/ is capable of distinguishing between just two classes. An example of this is the /Stochastic Gradient Descent/ (SGD) classifier, which can handle very large datasets efficiently, and deals with training instances independently, one at a time. The "stochastic" in the name means that it relies on randomness during training.

*** Performance Measures

Evaluating a classifier is often trickier than evaluating a regressor!

~cross_val_predict()~ performs K-fold cross-validation and returns the predictions (instead of the score) made on each test fold.

**** Accuracy

Ratio of correct predictions. It's generally not the preferred performance measure for classifiers, especially wen dealing with /skewed datasets/ (when some classes have much more instances than others).

**** Confusion Matrix

Often a much better way to evaluate the performance of a classifier than the accuracy, the confusion matrix allows us to gain some insights on where exactly the classifier is going wrong (or right!).

**** Precision

Measures the accuracy of the positive predictions. It is calculated by

$$
\rm{precision} = \frac{TP}{TP + FP}
$$

$TP$ is the number of true positives, and $FP$ is the number of false positives.

**** Recall

Also named /sensitity/ or the /true positive rate/ (TPR), it is the ratio of positive instances that are correctly detected by the classifier. It is given by

$$
\rm{recall} = \frac{TP}{TP + FN}
$$

$FN$ is the number of false negatives.

**** F1 score

The $F_1$ /score/ is the combination of precision and recall into a single metric. This score is given by the /harmonic mean/ of precision and recall. Remember that the harmonic mean gives much more weight to low values! This effectively means that a classifier will only get a high $F_1$ score if both recall and precision are high. It is given by

$$
2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
$$

This score is overall a great way to measure a classifiers' performance, but in some context we might, for example, care more about precision than recall, so this score is no panacea!

Keep in mind that increasing precision reduces recall, and vice versa. This is called the /precision/recall trade-off/. Also, a high-precision classifier is not very useful if its recall is too low!

**** ROC Curve

The /receiver operating characteristic/ (ROC) curve is another very common tool used with binary classifiers. The ROC curve  plots the /true positive rate/ (recall) against the /false positive rate/ (FPR). The FPR is equal to 1 - /true negative rate/ (TNR). The TNR is also called /specificity/, so the ROC curve plots /sensitivity/ versus 1 - /specificity/.

Here we have yet another trade-off: the higher the recall, the more false positives the classifier produces.

We can use the ROC curve to compare classifiers by measuring the /area under the curve/ (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.

Between the ROC curve and the PR curve, the latter should be preferred whenever the positive class is rare or when we care more about false positives than the false negatives. Otherwise, we should use the ROC curve.

*** Multiclass Classification

/Multiclass classifiers/ (also called /multinomial classifiers/) can distinguish between more than two classes. There are algorithms capable of handling multiple classes natively (such as Random Forest classifiers and naive Bayes classifiers). Others (such as Support Vector Machines or Logistic Regression) are strictly binary classifiers. However, there are strategies we can use to perform multiclass classification with multiple binary classifiers.

There is the /one-versus-the-rest/ (OvR) strategy (also called /one-versus-all/): we train $N$ binary classifiers (with $N$ being the number of classes), each of which is capable of identifying if a given sample is of a particular class. To classify a new instance, we use the assignment of the classifier with the highest score.

We can also train a binary classifier for every pair of classes. This is called the /one-versus-one/ (OvO) strategy. For $N$ classes, we would need to train $N \times (N - 1) / 2$ classifiers! The main advantage of this strategy is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish. For algorithms that scale poorly with the size of the training set (such as Support Vector Machine), it is actually faster to train many classifiers on small training sets than to train few classifiers on large training sets! However, for most binary classification algorithms, OvR is preferred.

Scikit-Learn automatically detects when we are trying to use a binary classification algorithm for a multiclass classification problem, and runs OvR or OvO for us depending on the algorithm. We can specify which strategy we prefer by using the ~OneVsOneClassifier~ or ~OneVsRestClassifier~ classes.

*** Error Analysis

Assuming we have found a promising model and are looking for ways to improve it, analyzing the types of errors it makes might be a great bet! We could start looking at the confusion matrix to gain some insights on how to improve the classifier. If we notice that the model is biased towards some specific classes, we would have a very clear objective in mind to do, for example, some feature engineering in order to mitigate the problem.

*** Multilabel Classification

In some cases we may want to have a classifier capable of assigning multiple classes for each instance. Such a classification system is called a /multilabel classification/ system. There are many ways to evaluate a multilabel classifier. One approach would be to measure the $F_1$ score for each individual label, then simply average it out. However, this assumes that all labels are equally important, which may not be the case. If it's not, we could give each label a weight equal to its /support/ (the number of instances with that target label).

*** Multioutput Classification

/Multioutput-multiclass classification/ (or simply /multioutput-classification/) is a generalization of multilabel classification, where each label can be multiclass.

** 04. Training Models

*** Linear Regression

A linear regression model makes predictions by computing a weighted sum of the input features, plus a constant called the /bias term/ (or /intercept term/). This can be written as

$$
\hat y  = h_{\theta}(\bm{x}) = \bm{\theta} \dot \bm{x}
$$

where $\bm{\theta}$ is the model's /parameter vector/, $\bm{x}$ is the /feature vector/, and $h_{\theta}$ is the hypothesis function.

Note: In Machine Learning, vectors are often represented as /column vectors/, which are 2D arrays with a single column.

**** The Normal Equation

To find the value of $\bm{\theta}$ that minimizes the cost function, there is a /closed-form/ solution: a mathematical equation that gives the result directly. This is called the /Normal Equation/:

$$
\bm{\hat \theta} = (\bm{X}^T\bm{X})^{-1}\bm{X}^T\bm{y}
$$

where $\bm{\hat \theta}$ is the value of $\bm{\theta}$ that minimizes the cost function, $\bm{y}$ is the vector of target values, and $\bm{X}$ is the training data.

Scikit-Learn's ~LinearRegression~ class is based on the ~scipy.linalg.lstsq()~ (least squares) function, which computes $\bm{\hat \theta}$ using the /pseudoinverse/ of $\bm{X}$ (more specifically, the Moore-Penrose inverse). The pseudoinverse itself is computed using /Singular Value Decomposition/ (SVD). This approach is more efficient than computing the Normal Equation, and has the advantage of handling edge cases nicely: the pseudoinverse is always defined (whereas the inverse matrix is not defined for singular matrices).

**** Computational Complexity

The /computational complexity/ of inverting a matrix $\bm{X}$ with $n$ features is typically about $\mathcal{O}(n^{2.4})$ to $\mathcal{O}(n^3)$, depending on the implementation. The SVD approach is about $\mathcal{O}(n^2)$. Both approaches can get very slow when the number of features grow large. The good thing is that both are linear with regard to the number of instances in the training set (they are $\mathcal{O}(m)$).

*** Gradient Descent

The general idea of this optimization algorithm is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regard to the parameter vector $\bm{\theta}$, and goes in the direction of the descending gradient. The minimum is reached when the gradient is zero. This algorithm performs a search in the model's /parameter space/: the more parameters it has, the harder the search is!

Warning: When using Gradient Descent, all features should have a similar scale, or else it will take much longer to converge!

Note: GD scales well with the number of features.

The MSE cost function for a Linear Regression model is a /convex function/, which implies that there are no local minima, just one global minimum! It is also a continuous function with a slope that never changes abruptly. These two facts are enough to guarantee that Gradient Descent will approach the global minimum.

**** Batch Gradient Descent

This implementation of Gradient Descent uses the whole batch of training data at every step, computing all the partial derivatives of the cost function. Consequently, it is terribly slow on very large training sets.

Setting the appropriate number of iterations is an important aspect of the Gradient Descent: a number too low, and the algorithm will still be far away from the optimal solution when it stops; if it is too high, time will be wasted after convergence with model parameters that do not change anymore. A solution to this is to set a very large number of iterations, but interrupt the algorithm when the gradient vector becomes too small (smaller than a tolerance $\epsilon$), indication that the algorithm has almost reached the minimum.

**** Stochastic Gradient Descent

Batch Gradient Descent has a major disadvantage: it uses the whole training set to compute the gradients at every step, which can be very computationally expensive. /Stochastic Gradient Descent/ deals with this issue by picking a random instance of data at every step, and computing the gradients based only on that single instance.

Due to its stochastic (i.e. random) nature, this algorithm is much less regular than the Batch variant: the cost function will bounce up and down, decreasing only on average. The algorithm never settles down, it will continue to bounce around even when it is already very close to the minimum. However, there are situations (when the cost function is very irregular) in which this behavior can actually help the algorithm jump out of local minima.

Although this randomness has its perks, it also means that the algorithm will never settle at the minimum. We can tackle this by gradually reducing the learning rate, which will make the algorithm bounce around less and less as it approaches the minimum. However, using an appropriate /learning schedule/ (the function that determines the learning rate at each iteration) is crucial: if we reduce the learning rate too quickly, the algorithm might end up frozen halfway to the solution; if we reduce the learning rate too slowly, the algorithm will still jump around for a long time, which can cause it to end up with a suboptimal solution.

Another aspect to be aware of is that since instances are picked randomly, some instances may be picked much more often than expected, while others may not be picked at all.

Warning: When using SGD, the instances must be independent and identically distributed (IID). If this is not the case, SGD may start by optimizing for one label, then the next, and so on, which will probably lead to a poor solution when it settles.

**** Mini-batch Gradient Descent

Mini-batch GD sits right between Batch GD and Stochastic GD: it computes the gradient on small random sets of instances called /mini-batches/. Mini-batch GD has a major advantage over Stochastic GD: it can get a performance boost from hardware optimization of matrix operations!

Compared to Stochastic GD, Mini-batch GD will end up walking closer to the minimum, but it may be harder for it to escape from local minima.

The following table sums up pretty nicely the comparison between algorithms for Linear Regression:

| *Algorithm*     | *Large m* | *Out-of-core support* | *Large n* | *Hyperparameters* | *Scaling required* | *Scikit-Learn*     |
|-----------------+-----------+-----------------------+-----------+-------------------+--------------------+--------------------|
| Normal Equation | Fast      | No                    | Slow      |                 0 | No                 | N/A                |
| SVD             | Fast      | No                    | Slow      |                 0 | No                 | ~LinearRegression~ |
| Batch GD        | Slow      | No                    | Fast      |                 2 | Yes                | ~SGDRegressor~     |
| Stochastic GD   | Fast      | Yes                   | Fast      |                ≥2 | Yes                | ~SGDRegressor~     |
| Mini-batch GD   | Fast      | Yes                   | Fast      |                ≥2 | Yes                | ~SGDRegressor~     |

*** Polynomial Regression

Although a linear model is, well, linear, it can be used to fit nonlinear data! A simple way to achieve this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called /Polynomial Regression/.

*** Learning Curves

Learning curves are plots of the model's performance on the training set and the validation set as a function of the training set size (or the training iteration). Plotting such curves can help to analyze the model's behavior, such that identifying under and overfitting becomes easy.

*** The Bias/Variance Trade-Off

A model's generalization error can be expressed as a sum of three very different errors:

- /Bias/: This part of the error is due to wrong assumptions (such as assuming that the data distribution is simpler than it actually is). A high-bias model is likely to underfit the training data.
- /Variance/: This part is due to the model's excessive sensitivity to small variations in the training data. A model with many degrees of freedom is likely to have high variance and thus overfit the data.
- /Irreducible error/: This is due to the noisiness of the data itself. The only way to reduce this error is to clean up the data.

  Increasing a model's complexity will typically increase its variance and reduce its bias. Conversely, reducing a model's complexity will increase its bias and reduce its variance.

*** Regularized Linear Models

A good way to reduce overfitting is to regularize the model: the fewer degrees of freedom it has, the harder it will be for it to overfit the data. When dealing with linear models, regularization is often achieved by constraining the weights of the model.

**** Ridge Regression

/Ridge Regression/ (also called /Tikhonov regularization/) is a regularized version of Linear Regression. The difference is that the /regularization term/ $\alpha \sum_{i=1}^n \theta_i^2$ is added to the cost function. This forces the algorithm to keep the model weights as small as possible.

Note that the regularization term should only be added to the cost function during training! Once the model is trained, we want to use the unregularized performance measure to evaluate the model.

The parameter $\alpha$ controls the strength of the regularization: if $\alpha=0$, then Ridge Regression is just Linear Regression; if $alpha$ is very large, then all weights end up very close to zero, resulting in a flat line going through the data's mean.

Warning: Ridge Regression is sensitive to the scale of the input features, so it is important to regularize the data! This is true of most regularized models.

**** Lasso Regression

/Least Absolute Shrinkage and Selection Operator Regression/ (aka /Lasso Regression/) is another regularized version of Linear Model: it adds a regularization term to the cost function, but it uses the $\ell_1$ norm of the weight vector instead of half the square of the $\ell_2$ norm.

An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features by setting them to zero. This implies that Lasso Regression automatically performs feature selection and output a /sparse model/.

**** Elastic Net

Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a mix of both Ridge and Lasso's regularization term, and we can control the mix ratio $r$. When $r=0$ Elastic Net is equivalent to Ridge Regression, and when $r=1$, it is equivalent to Lasso Regression.

It is almost always preferable to have at least a little bit of regularization instead of using plain Linear Regression. Ridge is a good default, but if we have any reasons to suspect that only a few features are actually useful, Lasso or Elastic Net should be preferred, since they tend to reduce the useless features weights down to zero. In general, Elastic Net is preferred over Lasso, because the latter may behave erratically when the number of features is greater than the number of training instances, or when several features are strongly correlated.

**** Early Stopping

Iterative learning algorithms can be regularized in a very different way: we can stop the training as soon as the validation errors reaches a minimum. This is called /early stopping/.

*** Logistic Regression

/Logistic Regression/ is an example of a regression algorithm that can be used for classification: it is commonly used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than 50%, then the model predicts that the instance belongs to the /positive class/ (labeled "1"), and otherwise it predicts that it does not (belongs to the /negative class/, labeled "0"). This makes it a binary classifier!

**** Estimating Probabilities

A Logistic Regression model computes a weighted sum of the input features, but instead of outputting the result directly, it outputs the /logistic/ of this results. The logistic is a /sigmoid function/ that outputs a number between 0 and 1.

Once the Logistic Regression model has estimated the probability $\hat p = h_{\bm{\theta}}(\bm{x})$ that an instance $\bm{x}$ belongs to the positive class, it can make its prediction as so:

$$
\hat y = \begin{cases}
    0 & \text{if}\ \hat p < 0.5 \\
    1 & \text{if}\ \hat p \ge 0.5
\end{cases}
$$

**** Training and Cost Function

The cost function over the whole training set is the average cost over all training instances!

There is no known closed-form equation to compute the value of $\bm{\theta}$ that minimizes the cost function. Good news is that the cost function is convex, so we can use Gradient Descent (or any other optimization algorithm).

**** Softmax Regression

The Logistic Regression can be generalized to support multiple classes directly, without the need to train and combine multiple binary classifiers. This is called /Softmax Regression/, or /Multinomial Logistic Regression/.

When given an instance $\bm{x}$, the Softmax Regression model computes a score $s_k(\bm{x})$ for each class $k$, then estimates the probability of each class by applying the /softmax function/.

With the scores of every class computed for an instance $\bm{x}$, we can estimate the probability $\hat p_k$ that the instance belongs to the class $k$ by running the scores through the softmax function.

Cross entropy is frequently used to measure how well a set of estimated class probabilities matches the target classes.
