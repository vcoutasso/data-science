#+TITLE: Hands-On Machine Learning Notes

* Table of contents :TOC_2:
- [[#part-i---the-fundamentals-of-machine-learning][Part I - The Fundamentals of Machine Learning]]
  - [[#01-the-machine-learning-landscape][01. The Machine Learning Landscape]]
  - [[#02-end-to-end-machine-learning-project][02. End-to-End Machine Learning Project]]
  - [[#03-classification][03. Classification]]
  - [[#04-training-models][04. Training Models]]
  - [[#05-support-vector-machines][05. Support Vector Machines]]
  - [[#06-decision-trees][06. Decision Trees]]
  - [[#07-ensemble-learning-and-random-forests][07. Ensemble Learning and Random Forests]]
  - [[#08-dimensionality-reduction][08. Dimensionality Reduction]]
  - [[#09-unsupervised-learning-techniques][09. Unsupervised Learning Techniques]]
- [[#part-ii---neural-networks-and-deep-learning][Part II - Neural Networks and Deep Learning]]
  - [[#10-introduction-to-artificial-neural-networks-with-keras][10. Introduction to Artificial Neural Networks with Keras]]

* Part I - The Fundamentals of Machine Learning

** 01. The Machine Learning Landscape

*** What is Machine Learning?

Machine Learning is the science and art behind designing computer systems that can learn from data.

#+BEGIN_QUOTE
A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $P$, improves with experience $E$. -- Tom Mitchell, 1997
#+END_QUOTE

Applying ML techniques to data can help discovert patters that were not immediately apparent. This is called /data mining/, and is part of a process known as Knowledge Discovery in Databases (KDD).

*** Types of Machine Learning Systems

ML systems can be classified in broad categories, based on the following criteria:

- Whether or not human supervision is part of the training process (supervised, unsupervised, semi-supervised and reinforcement learning).
- If the system can learn incrementally on the fly (online learning) or not (batch learning).
- Works by comparing new data points to known data points (instance-based learning) or by detecting patterns in the training phase and building a predictive model (model-based learning).

**** Supervised learning

- Classification (classifies/labels new instances)
- Regression (predicts a target value for new instances)

**** Unsupervised learning

- Clustering (detects groups of similar data points)
- Anomaly/novelty detection (detects anomalies (outliers)/novelty classes)
- Data visualization and dimensionality reduction (creates lower-dimensional representations of the data)
- Association rule learning (discovers relations between attributes)

**** Semi-supervised learning

These methods (most of them combinations of supervised and unsupervised algorithms) can deal with partially labeled datasets (e.g. plenty of unlabeled instances, and few labeled instances).

**** Reinforcement learning

Trains an /agent/ which can perform actions and get /rewards/ (or /penalties/) in return. It learns by itself what is the best strategy (/policy/) to get the most reward over time.

**** Batch learning

The system is not capable of learning incrementally, thus it must be trained using all the available data first, so it can be launched into production (where it runs without learning anymore). This is called /offline learning/.
For a batch learning system to know about new data, a new version of it must be trained from scratch on the full (now augmented) dataset.

**** Online learning

As opposed to batch learning, these system are trained incrementally by feeding it data instances sequentially in groups called /mini-batches/. This strategy is most suitable for systems that need to adapt to change rapidly or autonomously. This idea can also be used to train systems on datasets too large to fit in memory (/out-of-core/ learning).

**** Instance-based learning

This form of learning can be thought of as simply learning by heart: the system learns the training instances by heart, then generalizes to new cases using a similarity measure to compare them to the learned examples.

**** Model-based learning

Another way to generalize from a training set is to build a model of these data points and then use that model to make /predictions/ on new data (this is called /inference/).

*** Challenges of Machine Learning

**** Insufficient Quantity of Training Data

This [[https://dl.acm.org/doi/10.3115/1073012.1073017][famous paper]] from 2001 showed that very different Machine Learning algorithms performed almost identically on a complex problem, once they were given enough data. Unfortunately, it is not always feasible to obtain extra data.

**** Non-representative Training Data

For a model to be successful, it is crucial that the data it was trained on is representative of the new cases it must generalize to. Some problems we might come across are: if the sample is too small, we can have /sampling noise/ (the data is non-representative as a result of chance); if the sampling method is flawed, even large samples can be non-representative. This is called /sampling bias/.

**** Poor-Quality Data

Preparing and cleaning up datasets takes a significant part of a data scientist's time. This is very important because if the training data is full of problems, it will make it much harder for the system to detect the underlying patterns.

**** Irrelevant Features

The process of coming up with a good set of features to train a model on is called /feature engineering/, and involves the following steps:

- /Feature selection/, which consists of selecting the most useful features for training.
- /Feature extraction/, which consists of combining existing features to produce more useful ones (dimensionality reduction can help with that).
- Creating new features by gathering new data.

**** Overfitting the Training Data

We can identify overfitting when a model performs well on the training data, but it does not generalize well for new instances. Complex models can detect subtle patterns in the data, but if the training set is too noisy or too small, the model is likely to detect patterns in the noise itself.

We can reduce the risk of overfitting by constraining a model, that is, applying /regularization/ to it. There are many different kinds regularization strategies, but they essentially seek to constrain the degree of freedom of a given model, basically ensuring simpler models.

The amount of regularization to apply during learning is controlled by a /hyperparameter/ that must be set prior to training and remains constant.

Options to fix this:

- Simplify the model (use fewer parameters, reduce the number of attributes in the data, or constrain the model).
- Use more training data.
- Reduce the noise in the data (fix data issues and remove outliers).

**** Underfitting the Training Data

Underfitting is the opposite of overfitting: if a model is too simple to learn the underlying structure of the data, its performance will be greatly reduced.

Options to fix this:

- Use a more powerful/complex model.
- Use better features (feature engineering).
- Reduce the constraints (regularization hyperparameter).

*** Testing and Validating

Split the data into two sets: the /training set/ and the /testing set/. It's common to use 80% of the data for training and /hold out/ 20% for testing.

The error rate on new cases is called the /generalization error/ (or /out-of-sample error/). By evaluating a model on the test set, we can get an estimate of this error.

**** Hyperparameter Tuning and Model Selection

If we evaluate the model on the test set too many times and use the results to "improve" it, we might be overfitting the model to the test data.

A common solution to this problem is the /holdout validation/: we hold out part of the training set (this new subset is called /validation set/) to evaluate candidate models and select the best one. After this validation process, we train the model on the full training set (including the validation set) to get the final model, which is then evaluated on the test set.

This works quite well, but can lead to problems: for a validation set too small we take our chances with sampling bias, since we can end up with a non-representative subset of the data. If the validation set is too large, the remaining training set will be much smaller than the full training set.

This issues can be solved by performing repeated /cross-validation/: we split the training set in $k$ subsets of equal size, and use each in turn as a validation set. We then average out all the evaluations of a model, resulting in a much more accurate measure of its performance. The drawback here is that the training time is multiplied by the number of validation sets $k$.

**** Data Mismatch

It might be the case in some applications that the data the model was trained on does not represent the data the model will work with in production. To avoid this, the validation and the test set must be as representative as possible of the data the model will use in production.

If there is a risk of mismatch, we can use yet another set (by holding out some of the training set) that Andrew Ng calls the /train-dev set/. The model is trained on the rest of the training set, and then evaluated on both the train-dev set and the validation set. If the model performs well on the training set, but not on the train-dev set, it is likely overfitting the training data.  If it performs well on both the training set and the train-dev set, but not on the validation set, there is probably some mismatch between the training data and the validation + test data.

** 02. End-to-End Machine Learning Project

*** Look at the Big Picture

**** Some terminology

- /Multiple regression/ problem: a problem in which the system uses multiple features to make a prediction.
- /Univariate regression/ problem: a problem in which we are only trying to predict a single value.
- /Multivariate regression/ problem: a problem in which we are trying to predict multiple values.
- /Hypothesis/: a machine learning system's prediction function may be called a hypothesis, usually denoted by $h$.

**** Performance Measure

- Root-mean-square deviation (RMSE) is generally the preferred performance measure for regressions tasks, although very sensible to outliers. It corresponds to the _Euclidean norm_, also called the $\ell_2$ /norm/,
   noted $\|\cdot\|_2$ (or just $\|\cdot\|$).
- Mean absolute error (MAE), also called the average absolute deviation, is a good option in the presence of outliers. It corresponds to the $\ell_1$ /norm/, noted $\|\cdot\|_1$. This is sometimes called the /Manhattan norm/.
- A /norm/ is a distance measure. The $\ell_k$ /norm/ of a vector $\bold{v}$ containing $n$ elements is defined as $\|\bold{v}\|_k = \left( |v_0|^k + |v_1|^k + \dots + |v_n|^k \right)^{\frac{1}{k}}$. $\ell_0$ gives the number of nonzero elements in the vector. $\ell_\infty$ gives the maximum absolute value in the vector.
- The higher the norm index $k$, the more if focuses on large values in detriment of smaller ones. That why RMSE ($\ell_2$) is more sensitive to outliers than MAE ($\ell_1$). However, if outliers are exponentially rare, RMSE still performs very well and is generally preferred.

*** Get the Data

A /tail-heavy/ histogram extends much farther to the right of the median than to the left. Feature distributions such as this may make it harder for some ML algorithms to detect patterns. When possible, consider transforming these features (by computing their logarithm, for example).

**** Check the Assumptions!

It's good practice to thoroughly list and verify the assumptions made about the problem at hand. This can help catch serious issues early on, possibly preventing some gigantic headaches.

**** Test Set

This subject is extremely delicate and incorrect handling of test data may lead to creating (and worse: deploying) biased models. Some common mistakes to be aware of:

- Estimating the generalization error using the test set may lead to very optimistic (and quite possibly unrealistic) estimates. This is called /data snooping/ bias.
- The train/test split should be stable. If in every training iteration the data is split again, the model may get to see the whole dataset over time, which we want to avoid.
- Purely random sampling methods are generally fine if the dataset is large and balanced enough. If not, we run the risk of introducing a significant sampling bias.
- /Stratified sampling/ solves the issue of introducing sampling bias: the data is divided into homogeneous subgroups called /strata/, and the data is sampled in such a way that each stratum is guaranteed to be representative of the overall population. Notice that, if there is not a sufficient number of instances for each stratum, the estimate of a stratum's importance may be biased.

*** Discover and Visualize the Data

The ~jet~ color map ranges from blue to red, and it is great for visualizing density, for example.

**** Correlations

- The /standard correlation coefficient/ (also called /Pearson's r/) can be computed between every pair of attributes to discover linear correlation between them. This coefficient ranges from -1 to 1. When close to 1, it indicates strong positive correlation. When close to -1, it indicates strong negative correlation. Coefficients close to 0 mean that there is no linear correlation.
- Pandas' ~scatter_matrix()~ plots every numerical attribute against every other numerical attribute. The number of plots grows quadratically, so it might be a good idea to focus only on a few promising attributes depending on the dataset.

*** Prepare the Data

Using functions to prepare the data for ML algorithms is good practice. This allows for ease of reproduction, the habit may lead to a neat little library of common transformation functions, and the modularity allows for lots of flexibility when trying out different combinations of transformations.

**** Data Cleaning

Real world data rarely comes tidy and ready to be fed to ML algorithms: datasets often are filled with missing values among other problems. When dealing with missing values, we have three options:

1. Get rid of every sample that contain missing data.
2. Get rid of the whole attribute.
3. Set these values to some pre-determined value (e.g. zero, the mean, the median).

When working with the option 3, the median value (for example) should be computed using the training set to fill it. It's important to save these values for later use: they will be need to replace the missing values in the test set, as well as on new data when the system goes live. Scikit-Learn provides a handy class to take cara of missing values: ~SingleImputer~.

**** Text and Categorical Attributes

It's quite common for categorical attributes to be represented as text (e.g. low, normal, high). Most ML algorithms prefer to work with numbers, so we can convert these categories from text to numbers. Scikit-Learn's ~OrdinalEncoder~ is a great tool for just that!

One issue with that is that the algorithms will assume that two nearby numerical values are more similar than two distant values. We can avoid this by using what's called /one-hot encoding/, adding extra binary attributes that represent the categorical values. This is called /one-hot encoding/, and Scikit-Learn provides the ~OneHotEncoder~ class to do this.

**** Feature Scaling

ML algorithms generally don't perform well when the input attributes have very different scales. We have two main approaches to address this issue: /min-max scaling/ (also called /normalization/) and /standardization/:

- Normalization rescales the values so that they end up ranging from 0 to 1 (or some other arbitrary range).
- Standardization first subtracts the mean value, then it divides by the standard deviation. It does not bound values to some pre-determined range, but it's much less affected by outliers.

Important: the scalers should be fed *the training data only* to prevent any kind of bias.

*** Selecting and Training a Model

/K-fold cross-validation/ is usually a good strategy for a reliable evaluation of a model.

It is good practice to save models we experiment with. The /pickle/ module lets us do just that, serializing the model and saving it as a file. The /joblib/ library is another option, which is more efficient at serializing large NumPy arrays.

*** Fine-Tuning a Model

**** Grid Search and Randomized Search

~GridSearchCV~ is a neat little tool that searches for the best combination of hyperarameters for us, given a set of values to be tested. It uses cross-validation to evaluate all the possible combinations. One thing to keep in mind is that if the best value for a given hyperparameter is the largest value of the range of possibilites supplied, it might be a good idea to search again with higher values (we might find something even better!).

The problem with the grid search approach is that it's very computationally expensive: a model is trained once for every single combination of hyperparameters. With a model complex enough and many combinations to test, the task can grow to become intractable in reasonable time pretty quickly. For occasions like this, it is often preferable to use ~RandomizedSearchCV~ instead. It evaluates a given number of random combinations, instead of all of them. With this we have much more control of how much time we spend.

**** Analyze the Best Models

We will often gain good insights on the problem by inspecting the best models. For example, the ~RandomForestRegressor~ estimator can indicate the relative of each attribute for making predictions!

**** Evaluate the System on the Test Set

This is the final step of creating a model, and the only moment we really deal with the test set.

Tip: If we want to have an idea of how precise and estimate is, we can compute a 95% /confidence interval/ for the generalization error using ~scipy.stats.t.interval()~.

*** Launching, Monitoring and Maintaining a System

The fact is, we need to monitor a model's live performance. Relevant processes may fail (we need to be prepared for dealing with those), performance may degrade because of a poor-quality input signal (we could monitor inputs somehow to detect these), and data that keeps evolving may render a model useless over time.

It's important to keep backups of every model used, as well as the tools to properly and quickly work with them.

** 03. Classification

*** MNIST

The MNIST dataset is a set of 70,000 small images of handwritten digits. This set has been studied so much that it is often called the "hello world" of Machine Learning. Each image is 28 $\times$ 28 pixels (totaling 784), and this dataset is already split in a training set (the first 60,000 images) and a test set (the last 10,000 images).

Some ML algorithms are sensitive to the order of training instances, so feeding them many similar instances in a row might affect performance. Shuffling the dataset beforehand is a good idea since it ensures that this won't happen.

*** Training a Binary Classifier

A /binary classifier/ is capable of distinguishing between just two classes. An example of this is the /Stochastic Gradient Descent/ (SGD) classifier, which can handle very large datasets efficiently, and deals with training instances independently, one at a time. The "stochastic" in the name means that it relies on randomness during training.

*** Performance Measures

Evaluating a classifier is often trickier than evaluating a regressor!

~cross_val_predict()~ performs K-fold cross-validation and returns the predictions (instead of the score) made on each test fold.

**** Accuracy

Ratio of correct predictions. It's generally not the preferred performance measure for classifiers, especially wen dealing with /skewed datasets/ (when some classes have much more instances than others).

**** Confusion Matrix

Often a much better way to evaluate the performance of a classifier than the accuracy, the confusion matrix allows us to gain some insights on where exactly the classifier is going wrong (or right!).

**** Precision

Measures the accuracy of the positive predictions. It is calculated by

$$
\rm{precision} = \frac{TP}{TP + FP}
$$

$TP$ is the number of true positives, and $FP$ is the number of false positives.

**** Recall

Also named /sensitity/ or the /true positive rate/ (TPR), it is the ratio of positive instances that are correctly detected by the classifier. It is given by

$$
\rm{recall} = \frac{TP}{TP + FN}
$$

$FN$ is the number of false negatives.

**** F1 score

The $F_1$ /score/ is the combination of precision and recall into a single metric. This score is given by the /harmonic mean/ of precision and recall. Remember that the harmonic mean gives much more weight to low values! This effectively means that a classifier will only get a high $F_1$ score if both recall and precision are high. It is given by

$$
2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
$$

This score is overall a great way to measure a classifiers' performance, but in some context we might, for example, care more about precision than recall, so this score is no panacea!

Keep in mind that increasing precision reduces recall, and vice versa. This is called the /precision/recall trade-off/. Also, a high-precision classifier is not very useful if its recall is too low!

**** ROC Curve

The /receiver operating characteristic/ (ROC) curve is another very common tool used with binary classifiers. The ROC curve  plots the /true positive rate/ (recall) against the /false positive rate/ (FPR). The FPR is equal to 1 - /true negative rate/ (TNR). The TNR is also called /specificity/, so the ROC curve plots /sensitivity/ versus 1 - /specificity/.

Here we have yet another trade-off: the higher the recall, the more false positives the classifier produces.

We can use the ROC curve to compare classifiers by measuring the /area under the curve/ (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.

Between the ROC curve and the PR curve, the latter should be preferred whenever the positive class is rare or when we care more about false positives than the false negatives. Otherwise, we should use the ROC curve.

*** Multiclass Classification

/Multiclass classifiers/ (also called /multinomial classifiers/) can distinguish between more than two classes. There are algorithms capable of handling multiple classes natively (such as Random Forest classifiers and naive Bayes classifiers). Others (such as Support Vector Machines or Logistic Regression) are strictly binary classifiers. However, there are strategies we can use to perform multiclass classification with multiple binary classifiers.

There is the /one-versus-the-rest/ (OvR) strategy (also called /one-versus-all/): we train $N$ binary classifiers (with $N$ being the number of classes), each of which is capable of identifying if a given sample is of a particular class. To classify a new instance, we use the assignment of the classifier with the highest score.

We can also train a binary classifier for every pair of classes. This is called the /one-versus-one/ (OvO) strategy. For $N$ classes, we would need to train $N \times (N - 1) / 2$ classifiers! The main advantage of this strategy is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish. For algorithms that scale poorly with the size of the training set (such as Support Vector Machine), it is actually faster to train many classifiers on small training sets than to train few classifiers on large training sets! However, for most binary classification algorithms, OvR is preferred.

Scikit-Learn automatically detects when we are trying to use a binary classification algorithm for a multiclass classification problem, and runs OvR or OvO for us depending on the algorithm. We can specify which strategy we prefer by using the ~OneVsOneClassifier~ or ~OneVsRestClassifier~ classes.

*** Error Analysis

Assuming we have found a promising model and are looking for ways to improve it, analyzing the types of errors it makes might be a great bet! We could start looking at the confusion matrix to gain some insights on how to improve the classifier. If we notice that the model is biased towards some specific classes, we would have a very clear objective in mind to do, for example, some feature engineering in order to mitigate the problem.

*** Multilabel Classification

In some cases we may want to have a classifier capable of assigning multiple classes for each instance. Such a classification system is called a /multilabel classification/ system. There are many ways to evaluate a multilabel classifier. One approach would be to measure the $F_1$ score for each individual label, then simply average it out. However, this assumes that all labels are equally important, which may not be the case. If it's not, we could give each label a weight equal to its /support/ (the number of instances with that target label).

*** Multioutput Classification

/Multioutput-multiclass classification/ (or simply /multioutput-classification/) is a generalization of multilabel classification, where each label can be multiclass.

** 04. Training Models

*** Linear Regression

A linear regression model makes predictions by computing a weighted sum of the input features, plus a constant called the /bias term/ (or /intercept term/). This can be written as

$$
\hat y  = h_{\theta}(\bm{x}) = \bm{\theta} \dot \bm{x}
$$

where $\bm{\theta}$ is the model's /parameter vector/, $\bm{x}$ is the /feature vector/, and $h_{\theta}$ is the hypothesis function.

Note: In Machine Learning, vectors are often represented as /column vectors/, which are 2D arrays with a single column.

**** The Normal Equation

To find the value of $\bm{\theta}$ that minimizes the cost function, there is a /closed-form/ solution: a mathematical equation that gives the result directly. This is called the /Normal Equation/:

$$
\bm{\hat \theta} = (\bm{X}^\top\bm{X})^{-1}\bm{X}^\top\bm{y}
$$

where $\bm{\hat \theta}$ is the value of $\bm{\theta}$ that minimizes the cost function, $\bm{y}$ is the vector of target values, and $\bm{X}$ is the training data.

Scikit-Learn's ~LinearRegression~ class is based on the ~scipy.linalg.lstsq()~ (least squares) function, which computes $\bm{\hat \theta}$ using the /pseudoinverse/ of $\bm{X}$ (more specifically, the Moore-Penrose inverse). The pseudoinverse itself is computed using /Singular Value Decomposition/ (SVD). This approach is more efficient than computing the Normal Equation, and has the advantage of handling edge cases nicely: the pseudoinverse is always defined (whereas the inverse matrix is not defined for singular matrices).

**** Computational Complexity

The /computational complexity/ of inverting a matrix $\bm{X}$ with $n$ features is typically about $\mathcal{O}(n^{2.4})$ to $\mathcal{O}(n^3)$, depending on the implementation. The SVD approach is about $\mathcal{O}(n^2)$. Both approaches can get very slow when the number of features grow large. The good thing is that both are linear with regard to the number of instances in the training set (they are $\mathcal{O}(m)$).

*** Gradient Descent

The general idea of this optimization algorithm is to tweak parameters iteratively in order to minimize a cost function. Gradient Descent measures the local gradient of the error function with regard to the parameter vector $\bm{\theta}$, and goes in the direction of the descending gradient. The minimum is reached when the gradient is zero. This algorithm performs a search in the model's /parameter space/: the more parameters it has, the harder the search is!

Warning: When using Gradient Descent, all features should have a similar scale, or else it will take much longer to converge!

Note: GD scales well with the number of features.

The MSE cost function for a Linear Regression model is a /convex function/, which implies that there are no local minima, just one global minimum! It is also a continuous function with a slope that never changes abruptly. These two facts are enough to guarantee that Gradient Descent will approach the global minimum.

**** Batch Gradient Descent

This implementation of Gradient Descent uses the whole batch of training data at every step, computing all the partial derivatives of the cost function. Consequently, it is terribly slow on very large training sets.

Setting the appropriate number of iterations is an important aspect of the Gradient Descent: a number too low, and the algorithm will still be far away from the optimal solution when it stops; if it is too high, time will be wasted after convergence with model parameters that do not change anymore. A solution to this is to set a very large number of iterations, but interrupt the algorithm when the gradient vector becomes too small (smaller than a tolerance $\epsilon$), indication that the algorithm has almost reached the minimum.

**** Stochastic Gradient Descent

Batch Gradient Descent has a major disadvantage: it uses the whole training set to compute the gradients at every step, which can be very computationally expensive. /Stochastic Gradient Descent/ deals with this issue by picking a random instance of data at every step, and computing the gradients based only on that single instance.

Due to its stochastic (i.e. random) nature, this algorithm is much less regular than the Batch variant: the cost function will bounce up and down, decreasing only on average. The algorithm never settles down, it will continue to bounce around even when it is already very close to the minimum. However, there are situations (when the cost function is very irregular) in which this behavior can actually help the algorithm jump out of local minima.

Although this randomness has its perks, it also means that the algorithm will never settle at the minimum. We can tackle this by gradually reducing the learning rate, which will make the algorithm bounce around less and less as it approaches the minimum. However, using an appropriate /learning schedule/ (the function that determines the learning rate at each iteration) is crucial: if we reduce the learning rate too quickly, the algorithm might end up frozen halfway to the solution; if we reduce the learning rate too slowly, the algorithm will still jump around for a long time, which can cause it to end up with a suboptimal solution.

Another aspect to be aware of is that since instances are picked randomly, some instances may be picked much more often than expected, while others may not be picked at all.

Warning: When using SGD, the instances must be independent and identically distributed (IID). If this is not the case, SGD may start by optimizing for one label, then the next, and so on, which will probably lead to a poor solution when it settles.

**** Mini-batch Gradient Descent

Mini-batch GD sits right between Batch GD and Stochastic GD: it computes the gradient on small random sets of instances called /mini-batches/. Mini-batch GD has a major advantage over Stochastic GD: it can get a performance boost from hardware optimization of matrix operations!

Compared to Stochastic GD, Mini-batch GD will end up walking closer to the minimum, but it may be harder for it to escape from local minima.

The following table sums up pretty nicely the comparison between algorithms for Linear Regression:

| *Algorithm*     | *Large m* | *Out-of-core support* | *Large n* | *Hyperparameters* | *Scaling required* | *Scikit-Learn*     |
|-----------------+-----------+-----------------------+-----------+-------------------+--------------------+--------------------|
| Normal Equation | Fast      | No                    | Slow      |                 0 | No                 | N/A                |
| SVD             | Fast      | No                    | Slow      |                 0 | No                 | ~LinearRegression~ |
| Batch GD        | Slow      | No                    | Fast      |                 2 | Yes                | ~SGDRegressor~     |
| Stochastic GD   | Fast      | Yes                   | Fast      |                ≥2 | Yes                | ~SGDRegressor~     |
| Mini-batch GD   | Fast      | Yes                   | Fast      |                ≥2 | Yes                | ~SGDRegressor~     |

*** Polynomial Regression

Although a linear model is, well, linear, it can be used to fit nonlinear data! A simple way to achieve this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called /Polynomial Regression/.

*** Learning Curves

Learning curves are plots of the model's performance on the training set and the validation set as a function of the training set size (or the training iteration). Plotting such curves can help to analyze the model's behavior, such that identifying under and overfitting becomes easy.

*** The Bias/Variance Trade-Off

A model's generalization error can be expressed as a sum of three very different errors:

- /Bias/: This part of the error is due to wrong assumptions (such as assuming that the data distribution is simpler than it actually is). A high-bias model is likely to underfit the training data.
- /Variance/: This part is due to the model's excessive sensitivity to small variations in the training data. A model with many degrees of freedom is likely to have high variance and thus overfit the data.
- /Irreducible error/: This is due to the noisiness of the data itself. The only way to reduce this error is to clean up the data.

  Increasing a model's complexity will typically increase its variance and reduce its bias. Conversely, reducing a model's complexity will increase its bias and reduce its variance.

*** Regularized Linear Models

A good way to reduce overfitting is to regularize the model: the fewer degrees of freedom it has, the harder it will be for it to overfit the data. When dealing with linear models, regularization is often achieved by constraining the weights of the model.

**** Ridge Regression

/Ridge Regression/ (also called /Tikhonov regularization/) is a regularized version of Linear Regression. The difference is that the /regularization term/ $\alpha \sum_{i=1}^n \theta_i^2$ is added to the cost function. This forces the algorithm to keep the model weights as small as possible.

Note that the regularization term should only be added to the cost function during training! Once the model is trained, we want to use the unregularized performance measure to evaluate the model.

The parameter $\alpha$ controls the strength of the regularization: if $\alpha=0$, then Ridge Regression is just Linear Regression; if $alpha$ is very large, then all weights end up very close to zero, resulting in a flat line going through the data's mean.

Warning: Ridge Regression is sensitive to the scale of the input features, so it is important to regularize the data! This is true of most regularized models.

**** Lasso Regression

/Least Absolute Shrinkage and Selection Operator Regression/ (aka /Lasso Regression/) is another regularized version of Linear Model: it adds a regularization term to the cost function, but it uses the $\ell_1$ norm of the weight vector instead of half the square of the $\ell_2$ norm.

An important characteristic of Lasso Regression is that it tends to eliminate the weights of the least important features by setting them to zero. This implies that Lasso Regression automatically performs feature selection and output a /sparse model/.

**** Elastic Net

Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a mix of both Ridge and Lasso's regularization term, and we can control the mix ratio $r$. When $r=0$ Elastic Net is equivalent to Ridge Regression, and when $r=1$, it is equivalent to Lasso Regression.

It is almost always preferable to have at least a little bit of regularization instead of using plain Linear Regression. Ridge is a good default, but if we have any reasons to suspect that only a few features are actually useful, Lasso or Elastic Net should be preferred, since they tend to reduce the useless features weights down to zero. In general, Elastic Net is preferred over Lasso, because the latter may behave erratically when the number of features is greater than the number of training instances, or when several features are strongly correlated.

**** Early Stopping

Iterative learning algorithms can be regularized in a very different way: we can stop the training as soon as the validation errors reaches a minimum. This is called /early stopping/.

*** Logistic Regression

/Logistic Regression/ is an example of a regression algorithm that can be used for classification: it is commonly used to estimate the probability that an instance belongs to a particular class. If the estimated probability is greater than 50%, then the model predicts that the instance belongs to the /positive class/ (labeled "1"), and otherwise it predicts that it does not (belongs to the /negative class/, labeled "0"). This makes it a binary classifier!

**** Estimating Probabilities

A Logistic Regression model computes a weighted sum of the input features, but instead of outputting the result directly, it outputs the /logistic/ of this results. The logistic is a /sigmoid function/ that outputs a number between 0 and 1.

Once the Logistic Regression model has estimated the probability $\hat p = h_{\bm{\theta}}(\bm{x})$ that an instance $\bm{x}$ belongs to the positive class, it can make its prediction as so:

$$
\hat y = \begin{cases}
    0 & \text{if}\ \hat p < 0.5 \\
    1 & \text{if}\ \hat p \ge 0.5
\end{cases}
$$

**** Training and Cost Function

The cost function over the whole training set is the average cost over all training instances!

There is no known closed-form equation to compute the value of $\bm{\theta}$ that minimizes the cost function. Good news is that the cost function is convex, so we can use Gradient Descent (or any other optimization algorithm).

**** Softmax Regression

The Logistic Regression can be generalized to support multiple classes directly, without the need to train and combine multiple binary classifiers. This is called /Softmax Regression/, or /Multinomial Logistic Regression/.

When given an instance $\bm{x}$, the Softmax Regression model computes a score $s_k(\bm{x})$ for each class $k$, then estimates the probability of each class by applying the /softmax function/.

With the scores of every class computed for an instance $\bm{x}$, we can estimate the probability $\hat p_k$ that the instance belongs to the class $k$ by running the scores through the softmax function.

Cross entropy is frequently used to measure how well a set of estimated class probabilities matches the target classes.

** 05. Support Vector Machines

/Support Vector Machines/ are powerful and versatile ML models capable of performing linear and nonlinear classification, regression, and outlier detection. SVMs are particularly well suited for classification of complex datasets of small or medium sizes.

*** Linear SVM Classification

An intuitive way to think about an SVM classifier is as a model that fits the widest possible "street" between classes. This is called /large margin classification/.

Adding training instances "off the streets" will not affect the decision boundary at all: this is fully determined by the instances located on the edges of the street. These instances are called /support vectors/.

Warning: SVMs are sensitive to the feature scales.

**** Soft Margin Classification

We can strictly impose that all instances must be off the street and on the right side (with respect to the instances' class): this is called /hard margin classification/. This approach has two main issues: it only works if the data is intrinsically linearly separable; also, it is very sensitive to outliers (a single outlier could potentially break the model).

To avoid these issues, we can use a more flexible model. The objective of such model is to find a good balance between keeping the street as large as possible and limiting the /margin violations/. This is called /soft margin classification/.

Tip: If a SVM model is overfitting, we can try regularizing it by reducing $C$, the parameter that controls the softness of the margin.

*** Nonlinear SVM Classification

Many datasets are not linearly separable, so a linear model by itself would not be of much help. One approach to handle this is to add more features, such as polynomial features.

**** Polynomial Kernel

Additional polynomial features can work great with many different ML algorithms. However, at a low polynomial degree, this method is not able to deal with very complex dataset, and with a high polynomial degree it creates a huge number of features, making the model too slow.

Luckily, when using SVMs we can apply a technique called the /kernel trick/. The kernel trick makes it possible to get the same result as if we had added many polynomial features, without actually having to add them. Since we don't actually add any features, there is no combinatorial explosion at all!

Tip: When using grid search to find the right hyperparameter values, it is often faster to first do a very coarse search, then a finer search around the best values found.

**** Similarity Features

Another technique is to add features computed using a /similarity function/, which measures how much each instance resembles a particular /landmark/. As an example, we could use the Gaussian /Radial Basis Function/ (RBF) as a similarity function.

The results of this approach will depend on the similarity function used, as well as the landmarks we have selected. For the landmark selection, we could create one at the location of each and every instance in the dataset. Doing so, many dimensions are created, which increases the chances that the transformed dataset will be linearly separable. However, this approach transforms a training set with $m$ instances and $n$ features into a training set with $m$ instances and $m$ features (assuming we drop the original features). In other words, if the training set is very large, we'll end up with an equally large number of features.

**** Gaussian RBF Kernel

Just as with the polynomial method, the similarity features method can be useful with any ML algorithm, but it may be computationally intractable to compute all the additional features. The kernel trick does its magic once again, making it possible to obtain similar results as if we had added many similarity features.

The Gaussian RBF kernel is not the only one, but it is perhaps the most common. Some kernels are specialized for specific data structures, which can be quite useful.

Tip: As a rule of thumb, the linear kernel is often a good first option. If the training set is not too large, the Gaussian RBF kernel works well in most cases.

**** Computational Complexity

The ~LinearSVC~ class does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features. The algorithm takes longer if we require very high precision (which is controlled by the tolerance hyperparameter $\epsilon$).

The ~SVC~ class supports the kernel trick, but it gets dreadfully slow when the number of training instances gets too large.

The following is a neat comparison of Scikit-Learn classes for SVM classification:

| *Class*         | *Time complexity*                                          | *Out-of-core support* | *Scaling required* | *Kernel trick* |
|-----------------+------------------------------------------------------------+-----------------------+--------------------+----------------|
| ~LinearSVC~     | $\mathcal{O}(m \times n)$                                  | No                    | Yes                | No             |
| ~SGDClassifier~ | $\mathcal{O}(m \times n)$                                  | Yes                   | Yes                | No             |
| ~SVC~           | $\mathcal{O}(m^2 \times n)$ to $\mathcal{O}(m^3 \times n)$ | No                    | Yes                | Yes            |

*** SVM Regression

SVM is quite a versatile algorithm: it also supports linear and nonlinear regression. The trick is to reverse the objective: SVM Regression tries to fit as many instances as possible /on/ the street while limiting margin violations (which in this case would be instances /off/ the street). The width of the street is controlled by a hyperparameter $\epsilon$.

For dealing with nonlinear regression tasks, we can use a kernelized SVM model.

Note: SVMs can also be used for outlier detection. The author does not delve into this use case, but recommends the Scikit-Learn documentation for more details.

*** Under the Hood

This section uses a convention that is more convenient (and more common) when dealing with SVMs: the bias term is called $b$, and the feature weights vector is called $\bm{w}$.

**** Decision Function and Predictions

The linear SVM classifier predicts the class of a new instance $\bm{x}$ by computing the decision function $\bm{w}\top\bm{x} + b$. If the result is positive, the predicted class $\hat{y}$ is the positive class (1), and otherwise it is the negative class (0).

For a model fitted on a two-dimensional dataset, its decision function will be a 2D plane. The decision boundary is the set of points where the decision function intercepts the data plane.

Training a linear SVM classifier is essentially finding the values of $\bm{w}$ and $b$ that make the margin as wise as possible while avoiding margin violations (hard margin) or limiting them (soft margin).

**** Training Objective

The slope of the decision function is equal to the norm of the weight vector, $|| \bm{w} ||$. Dividing the slope by 2 will multiply the margin by 2. The smaller the weight vector $\bm{w}$, the larger the margin.

In order to get a large margin, we want to minimize $|| \bm{w} ||$.

Note: In practice, we minimize $\frac{1}{2} || \bm{w} ||^2$ rather than minimizing $|| \bm{w} ||$, because the former has a simple derivative (just $\bm{w}$), while the latter is not differentiable at $\bm{w} = 0$. Optimization algorithms work much better on differentiable functions.

To get the soft margin objective, a /slack variable/ $\zeta^{(i)}$ is introduces for each instance. $\zeta^{(i)}$ measures how much the $i^{\text{th}}$ instance is allowed to violate the margin. With this, we now have two conflicting objectives: minimize the slack variables to reduce margin violations, and minimize $\frac{1}{2} || \bm{w} ||^2$ to increase the margin. This is where the $C$ hyperparameter kicks in: it allows us to define the trade-off between these two objectives.

**** Quadratic Programming

Both the hard margin and soft margin problems are convex quadratic optimization problems with linear constraints (/Quadratic Programming/ (QP) problems).

We can set the QP parameters in such a way that we get the hard margin linear SVM classifier objective (better detailed in the book). We could then train a hard margin linear SVM using an off-the-shelf QP solver, provided that we pass it the appropriate parameters. Similarly, we can use a QP solver to solve the soft margin problem.

**** The Dual Problem

Given a constrained optimization problem, known as the /primal problem/, it is possible to express a different but closely related problem, called its /dual problem/. Typically, the solution to the dual problem gives a lower bound to the solution of the primal problem, but under some conditions it can have the same solution. Fortunately, the SVM problem meets these conditions!

The dual problem is faster to solve than the primal one when the number of training instances is smaller than the number of features. Also, the dual problem makes the kernel trick possible, while the primal does not.

**** Kernelized SVMs

According to /Mercer's theorem/, if a function $K(\bm{a}, \bm{b})$ respects the /Mercer's conditions/, then there exists a function $\phi$ that maps $\bm{a}$ and $\bm{b}$ into another space such that $K(\bm{a}, \bm{b}) = \phi(\bm{a})^\top \phi(\bm{b})$. This means that we can use $K$ as a kernel because we know $\phi$ exists, even if we don't know what it is exactly.

This allows us to simply replace the transformations by their correspondent kernels, essentially skipping the transformation step. This results in strictly the same as if we had transformed the whole training set then fitted a linear SVM algorithm, but this trick makes the whole process much more efficient.

Note that some frequently used kernels (such as the sigmoid kernel) don't respect all of the Mercer's conditions, but they generally work well in practice.

These are some of the most commonly used kernels:

$$
\begin{align*}
    \text{Linear:} && K(\bm{a}, \bm{b}) = \bm{a}^\top \bm{b} \\
    \text{Polynomial:} && K(\bm{a}, \bm{b}) = (\gamma \bm{a}^\top \bm{b} + r)^d  \\
    \text{Gaussian RBF:} && K(\bm{a}, \bm{b}) = \exp(-\gamma || \bm{a} - \bm{b} ||^2) \\
    \text{Sigmoid:} && K(\bm{a}, \bm{b}) = \tanh(\gamma \bm{a}^\top \bm{b} + r) \\
\end{align*}
$$

**** Online SVMs

For linear SVM classifiers, one way of implementing an online SVM classifier is to use Gradient Descent (e.g. using ~SGDClassifier~) to minimize the cost function, which is derived from the primal problem. However, Gradient Descent converges much more slowly than the methods based on QP.

It is also possible to implement online kernelized SVMs. For large-scale nonlinear problems, consider using neural networks instead.

**** Hinge Loss

The function $\max(0, 1 - t)$ is called the /hinge loss/ function. It is equal to 0 when $t \ge 1$. Its derivative (slope) is equal to -1 if $t < 1$ and 0 if $t > 1$.

** 06. Decision Trees

/Decision Trees/ are powerful versatile algorithms that can perform both classification and regression tasks (even multioutput tasks). They are also the fundamental components of Random Forests.

Decision Trees are intuitive and often easy to interpret. Such models are often called /white box models/. Conversely, there are /black box models/, such as neural networks, which makes it hard to know what contributed to the model's predictions·

*** Making Predictions

Decision Trees require very little data preparation: they don't require feature scaling or centering at all.

The ~gini~ attribute of a node measures its /impurity/: a node is pure if all training instances it applies to belong to the same class.

Scikit-Learn uses the CART algorithm, which produces only binary trees (questions only have yes/no answers). Other algorithms such as ID3 can produce Decision Trees with nodes that have more than two children.

*** Estimating Class Probabilities

A Decision Tree can also estimate the probability that an instance belong to a particular class $k$. This is done by traversing the tree to find the node for this instance, and returning the ratio of training instances of class $k$ in this node.

*** The CART Training Algorithm

The /Classification and Regression Tree/ (CART) algorithm works by splitting the training set into two subsets using a single feature $k$ and threshold $t_k$. It searches for the pair $(k, t_k)$ that produces the purest subsets (weighted by their size).

Once the algorithm has split the training set in two, it splits the subsets using the same logic recursively. It stop once it reaches the maximum predetermined depth, or if it cannot find a split that reduces impurity.

The CART algorithm is a /greedy algorithm/: it greedily searches for an optimum solution, but is not guaranteed to find one. Finding the optimal tree is known to be an /NP-Complete/ problem (requires $\mathcal{O}(\exp(m))$ time), which is why we have to settle for a "reasonably good" solution.

*** Computational Complexity

Decision Trees tend to be approximately balanced, so traversing the tree requires going through roughly $\mathcal{O}(\log_2(m))$ nodes, which is also the overall prediction complexity.

The training algorithm requires comparing all the features, which results in a training complexity of $\mathcal{O}(n \times m \log_2(m))$.

*** Gini Impurity or Entropy?

In Machine Learning, entropy is frequently used as an impurity measure: the entropy of a set is zero when it contains instances of only one class.

When training Decision Trees, using either Gini impurity or entropy usually does not make a big difference: both will lead to similar models. Gini impurity is slightly faster to compute, so it makes for a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees.

*** Regularization Hyperparameters

Decision Trees make very few assumptions about the training data. If left unconstrained, it will adapt very well to the training data: so well, in fact, that this will often lead to overfitting. Such a model is called a /nonparametric model/, because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. Conversely, a /parametric model/ has a predetermined number of parameters, so its degree of freedom is limited from the get-go, reducing the risk of overfitting (but increasing the risk of underfitting).

To avoid overfitting the data, we need to restrict the Decision Tree's degree of freedom. Generally this is done by restricting the maximum depth of the model. There are a few other parameters that similarly restrict the shape of the Decision Tree: the minimum number of samples a node must have before it can be split, the minimum number of samples a leaf node must have, the maximum number of leaf nodes, and the maximum number of features that are evaluated for splitting at each node. All of these can be adjusted accordingly in order to regularize the model.

Note: There are algorithm that work by first training the Decision Tree without restrictions, and then /pruning/ unnecessary nodes. Standard statistical tests, such as the chi-squared test, are used to estimate the probability that the improvement is purely the result of chance.

*** Regression

Regression is very similar to classification with Decision Trees, with one key difference: instead of predicting a class in each node, the tree predicts a value. This prediction is the average target value of all the training instances associated with a particular leaf node. This means that the predicted value for each region is always the average target value of the instances in that region.

The CART algorithm described previously works very similarly as well, except that it now tries to split the training set in a way the minimizes the MSE.

Just like for classification tasks, Decision Trees are also prone to overfitting the training data when dealing with regression tasks.

*** Instability

One problem with Decision Trees is that they love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. This issue can be worked around using Principal Component Analysis (PCA), which often results in a better orientation of the training data.

More generally, Decision Trees are very sensitive to small variations in the training data. Removing a single instance of the training set (e.g., an outlier) can lead to /very/ different models.

** 07. Ensemble Learning and Random Forests

 If we aggregate the predictions of a group of predictors, we will often get better predictions than with the best individual predictor. This is called the /wisdom of the crowd/. A group of predictors is called an /ensemble/, and this technique is called /Ensemble Learning/, and an Ensemble Learning Algorithm is called an /Ensemble method/.

 As an example of an Ensemble method, we can train a groups of Decision Tree models, each on a different subset of the training data, and aggregate their outputs. Such an ensemble of Decision Trees is called a /Random forest/.

*** Voting Classifiers

We can aggregate the predictions of multiple classifiers and predict the class that gets the most votes. This majority-vote classifier is called a /hard-voting/ classifier.

Ensemble methods work best when the predictors are as independent from one another as possible (e.g., using very different algorithms). This decreases the chance that multiple classifiers will make the same types of errors.

If all classifiers are able to estimate class probabilities, such an ensemble will predict the class with the higher probability, averaged over all the individual classifiers. This is called /soft voting/. It often achieves better performance than hard voting because it gives more weight to highly confident votes (whereas in hard voting, a vote is a vote like every other).

*** Bagging and Pasting

Another approach would be to use the same algorithm for every predictor, but train them on different subsets of the data (yielding different predictors). When sampling is performed /with/ replacement, this method is called /bagging/. When sampling is performed /without/ replacement, it is called /pasting/.

After all predictors are trained, the ensemble can make predictions by simply aggregating the predictions of all predictors. The aggregation function is typically the /statistical mode/ (the most frequent prediction). Each individual predictor has a high bias (due to the smaller sample size), but this aggregation reduces both bias and variance on the final result. Generally, the ensemble has a similar bias but a lower variance than a single predictor trained on the full original set (it makes roughly the same number of errors, but the decision boundary is less regular).

These strategies allow for predictors that can all be trained in parallel, as well as predictions that can be made in parallel. This means that both bagging and pasting scale very well.

**** Out-of-Bag Evaluation

With bagging, due to the replacement, some instances may be sampled several times for any given predictor, while others may not be sampled at all. This means that only a portion (as $m$ grows, this approaches 63% of the instances) are samples for each predictor. The remaining instances are called /out-of-bag/ instances, and since they are never seen by the predictor, they can used for evaluation (without the need for a separate validation set!).

*** Random Patches and Random Subspaces

Sampling both training instances and features is called the /Random Patches/ method. Keeping all training instances but sampling features is called the /Random Subspaces/ method.

Sampling features results in more predictor diversity, trading some bias for a lower variance.

*** Random Forests

The Random Forest algorithm introduces randomness when growing trees: it searches for the best feature among a random subset of features. This results in greater tree diversity, which trades a higher bias for a lower variance, generally yielding an overall better model.

**** Extra-Trees

It is possible to make trees even more random by using random thresholds for each feature, rather than searching for the best possible thresholds like regular Decision Trees do. A forest of such random trees is called an /Extremely Randomized Trees/ ensemble (or /Extra-Trees/ for short). Extra-Trees are much faster to train than regular Random Forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree.

Tip: It is hard to tell in advance whether a Random Forest will perform better of worse than an Extra-Tree classifier. Generally, the only way to know is to compare them using cross-validation.

*** Feature Importance

Random Forests make it easy to measure the relative importance of each feature. We can measure a feature's importance by looking at how much the three nodes that use that particular feature reduce impurity on average (which is exactly what Scikit-Learn does). More precisely, it is a weighted average, where a node's weight is equal to the number of training instances that are associated with it.

Therefore, Random Forests are very useful to get a quick understand of what features actually matter, and can help us with feature selection.

*** Boosting

/Boosting/ refers to any Ensemble method that can combine several weak learners into a strong learner. The general idea is to train various predictors sequentially, each trying to correct the errors of its predecessor. The are many boosting methods available, but the most popular ones (and by far) are /AdaBoost/ and /Gradient Boosting/.

Warning: This sequential learning technique has the very important drawback of not being able to be parallelized (or at least partially).

**** AdaBoost

A predictor might correct its predecessor by paying more attention to the training instances that the predecessor underfitted. This is the technique used by AdaBoost.

AdaBoost starts by setting the weights of all instances to $\frac{1}{m}$. A first predictor is trained with these initial weights. Every predictor has a weight associated to it as well, which is directly proportional to how accurate the predictor is. Next, AdaBoost updates the instances weights, boosting the weights of the misclassified instances. This process is repeated until the desired number of predictors is achieved (a new predictor is trained on every step), or when a perfect predictor is found.

To make predictions, AdaBoost computes the predictions of all predictors, and weighs them using the respective predictor's weights. The predicted class is the one that receives the majority of weighted votes.

The default base estimator of the ~AdaBoostClassifier~ class is a Decision Stump, which is a tree composed of a single decision node plus two leaf nodes.

Tip: If AdaBoost is overfitting, it can be regularized by reducing the number of estimator, or more strongly regularizing the base estimator.

**** Gradient Boosting

Just like AdaBoost, Gradient Boosting also works by sequentially adding predictors to an ensemble, each one correcting its predecessor. The key difference is that this method tries to fit the new predictor to the /residual errors/ made by the previous predictor.

This Ensemble method with Decision Trees as base predictors for regression tasks is called /Gradient Tree Boosting/, or /Gradient Boosted Regression Trees/ (GBRT). This is implemented in the ~GradientBoostingRegressor~ class.

Scikit-Learns' ~GradientBoostRegressor~ supports a ~subsample~ hyperparameter, which specifies the fraction of training instances to be used for training each tree. This technique also trades a higher bias for a lower variance, and also speeds up the training considerably. This is called /Stochastic Gradient Boosting/.

An optimized implementation of Gradient Boosting is available in the popular Python library XGBoost, which stands for Extreme Gradient Boosting. In fact, XGBoost is often an important component of the winning entries in ML competitions. It is definitely worth checking out.

*** Stacking

Stacking is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictions in an ensemble, we can train a model to perform this aggregation. The final predictor (called a /blender/, or a /meta learner/), takes the predictions of the predictors as inputs and makes the final prediction.

To train a blender, a common approach is to use a hold-out set (alternatively, it is possible to use out-of-fold predictions). The predictions of the models can be used as input features to create a new training set, keeping the target values. The blender is then trained on this new training set, so it learns to predict the target values, given the previous layer's predictions.

This can be extended to train several blenders, in order to get a whole layer of blenders. Unfortunately, Scikit-Learn does not support stacking directly.

** 08. Dimensionality Reduction

Large datasets with many features often make training extremely slow. Not only that, but too many features can make it much harder to find a good solution. This problem is often referred to as the /curse of dimensionality/.

A possible solution to this is reducing the number of features using dimensionality reduction, which speeds up training and might turn an intractable problem into a tractable one. In some cases, reducing the dimensionality of the training data may even filter out noise, resulting in higher performance.

*** The Curse of Dimensionality

There is plenty of space in high dimensional spaces, which means that high-dimensional datasets are often at the risk of being very sparse: instances are likely to be far away from each other. Consequently, a new instance is also likely to be distant from all the known training instances, making predictions much less reliable. The more dimensions the training set has, the greater the risk of overfitting it!

Theoretically, it would be possible to increase the size of the training set to reach a sufficient density of training instances. In practice, the number of instances required grows exponentially with the number of dimensions, making this strategy impracticable in many cases.

*** Main Approaches for Dimensionality Reduction

**** Projection

In most real-world problems, training instances are not spread uniformly across all dimensions: they tend to lie within (or close to) a much-lower dimensional /subspace/ of the high-dimensional space. When this is the case, we can project every training instance onto this subspace in order to obtain a new, lower dimensional dataset.

However, projection is not always the right approach to dimensionality reduction. The famous /Swiss roll/ toy dataset is a clear example of this.

**** Manifold Learning

The Swiss roll dataset is an example of a 2D /manifold/. A 2D manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a $d$-dimensional manifold is a part of an $n$-dimensional space (where $d < n$) that locally resembles a $d$-dimensional hyperplane.

There are many dimensionality reduction algorithms that work by modeling the manifold on which the training instances lie. This is called /Manifold Learning/. This approach relies on the /manifold assumption/ (or /manifold hypothesis/), which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold (which is very often empirically observed). The manifold assumption is often accompanied by the implicit assumption that the task at hand will be simpler if expressed in the lower-dimensional space of the manifold (which is not always true!).

Here is a way to think about the manifold assumption: imagine we want to artificially create handwritten digits images; the degree of freedom available to us if we were to create any image we want is dramatically higher than the degree of freedom we would have to generate only digit images. These constraints tend to squeeze the dataset into a lower-dimensional manifold.

*** PCA

/Principal Component Analysis/ (PCA) is the most popular dimensionality reduction algorithm. It first identifies the hyperplane that lies closest to the data, then it projects the data onto it.

**** Preserving the Variance

Choosing the right hyperplane is a fundamental step of PCA. The strategy here is to favor the axis that minimizes the mean square distance between the original datasets and its projection onto that axis.

**** Principal Components

PCA identifies the axis that accounts for the largest amount of variance in the training set. the $i^{th}$ axis is called the $i^{th}$ /principal component/ (PC) of the data.

We can use /Singular Value Decomposition/ (SVD) to find these principal components: SVD can decompose the training set matrix $\rm{X}$ into the matrix multiplication of three matrices $\rm{U \Sigma V^\top}$, where $\rm{V}$ contains the unit vectors that define all the principal components.

Warning: PCA assumes that the data is centered around the origin. Scikit-Learn's PCA classes take care of this for us, but other libraries may not do the same.

**** Projecting Down to $d$ Dimensions

With all the principal componentes identified, we can reduce the dimensionality of the dataset down to $d$ by projecting it onto the hyperplane defined by the first $d$ principal components.

To project the data onto this hyperplane, we need to compute the matrix multiplication of the training set matrix $\rm{X}$ by the matrix $\rm{W}_d$, defined as the matrix containing the $d$ first columns of $\rm{V}$, as shown below:

$$
\rm{X}_{d\text{-proj}} = \rm{X}\rm{W}_d
$$

**** Explained Variance Ratio

The /explained variance ratio/ indicates the proportion of the dataset's variance that lies along each principal component.

**** Choosing the Right Number of Dimensions

We can choose the number of dimensions that explain a sufficiently large portion of the variance using the explained variance ratio.

**** PCA for Compression

After we have reduced the dimensionality of a dataset using PCA, we can "decompress" the reduced dataset to its original dimensionality by applying the inverse transformation of the PCA projection. This does not give us the original data, since the projection loses a bit of information, but it will likely be close to the original data. The mean square distance between the original data and the reconstructed data is called the /reconstruction error/.

**** Randomized PCA

/Randomized PCA/ is a stochastic algorithm that quickly finds an approximation of the first $d$ principal components (we can use this in Scikit-Learn by setting the ~svd_solver~ hyperparameter to ~"randomized"~). It is dramatically faster than the full SVD approach when $d$ is much smaller than $n$.

**** Incremental PCA

/Incremental PCA/ allows us to split the training set into mini-batches and feed it one mini-batch at a time. This is especially useful for large training sets and for applying PCA online. This is implemented in the ~IncrementalPCA~ class.

Alternatively, one can use NumPy's ~memmap~, which allows to manipulate large arrays stored on disk as if it were entirely in memory.

*** Kernel PCA

Turns out the we can also apply the kernel trick to PCA, allowing us to perform complex nonlinear projections for dimensionality reduction. This is called /Kernel PCA/ (kPCA).

**** Selecting a Kernel and Tuning Hyperparameters

Given that kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help us select the best kernel and hyperparameter values. However, when using it for a data preparation step in a supervised learning task, we can use grid search to select the kernel and hyperparameters that lead to the best performance on the task.

Another approach, which can be used in entirely unsupervised contexts, is to select the kernel and hyperparameters that yield the lowest reconstruction error. It is important to note, however, that reconstruction is not as easy as with linear PCA.

*** LLE

/Locally Linear Embedding/ (LLE) is a powerful /nonlinear dimensionality reduction/ technique. It works by first measuring how each training instance linearly relates to its closest neighbors, and then searching for a low-dimensional representation of the training set where these local relationships are preserved.

*** Other Dimensionality Reduction Techniques

*Random Projections*: Projects the data to a lower-dimensional space using a random linear projection. The quality of the dimensionality reduction depends on the number of instances and the target dimensionality.

*Multidimensional Scaling (MDS)*: Reduces dimensionality while trying to preserve the distances between the instances.

*Isomap*: Creates a graph by connecting each instance to its nearest neighbors, and then reduces dimensionality while trying to preserve the /geodesic distances/ (the number of nodes on the shortest path between two nodes in a graph).

*t-Distributed Stochastic Neighbor Embedding (t-SNE)*: Reduces dimensionality while trying to keep similar instances close to one another, and dissimilar instances apart. It is mostly used for visualization of instances in high-dimensional spaces (in particular to visualize clusters).

*Linear Discriminant Analysis (LDA)*: A classification algorithm that learns the most discriminative axes between classes, which can then be used to define a hyperplane onto which to project the data. This approach has the benefit of keeping classes as far apart as possible, so LDA is a good technique to reduce dimensionality before feeding the data to another classification algorithm.

** 09. Unsupervised Learning Techniques

In clustering, the goal is to group similar instances together into /clusters/. It's a great tool for data analysis, customer segmentation, recommender systems, search engines, image segmentation, and more.

The task of anomaly detection is about learning what "normal" data looks like, and then using this knowledge to detect abnormal instances.

Density estimation involves estimating the /probability density function/ (PDF) of the random process that generated the dataset. It is commonly used for anomaly detection: instances located in low-density regions are likely to be anomalies.

*** Clustering

Clustering is the task of identifying similar instances and assigning them to /clusters/ or groups of similar instances. Clustering covers a wide variety of applications, including:

*Customer segmentation*: This is used to cluster customers based on their purchases and activity. This is particularly useful when building /recommender systems/ to suggest content that other customers in the same cluster (group) enjoyed, for example.

*Data analysis*: It can be helpful to perform clustering on a dataset, and then analyze each cluster separately.

*Dimensionality reduction technique*: Once a dataset has been clustered, it is possible to measure the /affinity/ of each instance with each cluster. The affinity vector of an instance can be used to replace its feature vector, which is typically much higher-dimensional.

*Anomaly Detection*: (or /outlier detection/): Any instance that has low affinity to all the clusters is likely to be an anomaly.

*Semi-supervised learning*: When dealing with a dataset that only has a few labels, clustering can be performed to propagate the labels to all the instances in the same cluster.

*Search engines*: Clustering can be used in search engines to search for images that are similar to a reference image. A clustering algorithm can by applied to all images in a database, so that similar images would end up in the same cluster of the reference image.

*Image segmentation*: The number of different colors in a given image can be significantly reduced by clustering pixels according to their color, then replacing each pixel's color with the mean color of its cluster.

**** K-Means

K-Means is a simple algorithm capable of clustering datasets very quickly and efficiently. It is most suited for datasets with spherical clusters of similar sizes. It does not behave very well when the clusters have varying sizes, different densities or nonspherical shapes.

In the ~KMeans~ class, the ~transform()~ method measures the distance from each instance to every centroid.

The K-Means algorithm starts by randomly placing centroids, then it labels instances based on these centroids positions, and then updates the positions of the centroids with respect to the position of instances that belong to the same cluster. This process is repeated until the centroids stop moving. This algorithm is guaranteed to converge in a finite number of steps, but it may not converge to the right solution (depends on the centroid initialization).

The algorithm is usually repeated multiple times with different initializations, and the best solution is kept. The model with the lowest /inertia/ (mean squared distance between each instance and its closest centroid) is considered the best model.

K-Means++ introduces a smarter initialization step that tends to place centroids distant from one another. This makes the K-Means algorithm less likely to converge to suboptimal solutions.

If a dataset is too large to fit in memory, Mini-batch K-Means can be used. It converges much faster than the regular K-Means, but its inertia is generally worse (especially with high number of clusters).

The K-Means algorithm requires setting the number of centroids it must find, but this choice is not always obvious (especially when dealing with real-world datasets). To help with this, we can run the algorithm for various values of $k$, compute the /silhouette score/ (the mean /silhouette coefficient/ over all instances), and use the value of $k$ that maximizes this score.

We can obtain a very informative visualization of the performance of the algorithm on a given dataset by plotting every instance's silhouette coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This is called a /silhouette diagram/, and helps us identify promising values of $k$.

Tip: It is important to scale the input features before running K-Means. Although scaling does not guarantee that all the clusters will be nice and spherical, it generally improves performance.

**** Clustering for Image Segmentation

/Image segmentation/ is the task of partitioning an image into multiple segments. There are various kinds of image segmentation:

*Semantic segmentation*: All pixels that are part of the same object type get assigned to the same segment.

*Instance segmentation*: All pixels that are part of the same individual object are assigned to the same segment.

*Color segmentation*: All pixels are assigned to the same segment if they have a similar color.

**** DBSCAN

This clustering algorithm defines clusters as continuous regions of high density, and is capable of identifying any number of clusters of any arbitrary shape. It works well if all clusters are dense enough and if they are well separated by low-density regions.

It is robust to outliers and only has two hyperparameters.

There is a hierarchical variant of this algorithm called /Hierarchical DBSCAN/, which is implemented in the scikit-learn-contrib project.

**** Other Clustering Algorithms

*Agglomerative clustering*: This algorithm builds a hierarchy of clusters from the bottom up. This approach scales well with large numbers of instances or clusters. The algorithm is capable of capturing clusters of various shapes, and it produces a very informative cluster tree.

*BIRCH*: BIRCH builds a tree structure containing just enough information to assign each new instance to a cluster, without having to store all the instances in the tree. The advantage of this approach is that it uses limited memory while handling huge datasets.

*Mean-Shift*: This algorithm is capable of finding any number of clusters of any shape with very few hyperparameters to tune (it relies on local density estimation). Unlike DBSCAN, Mean-Shift tends to chop clusters into pieces when they have internal density variation. Due to its computational complexity, it is not suited for large datasets.

*Affinity propagation*: It uses a voting system, where each instance vote for similar instances to be their representatives. Once the algorithm converges, each representative and its voters form a cluster. It can detect any number of clusters of different sizes, but due to its computational complexity, it is not suited for large datasets.

*Spectral clustering*: This algorithm uses a similarity matrix between the instances to create a low-dimensional embedding from which, then it used another clustering algorithm in this low-dimensional space. The algorithm is capable of capturing complex cluster structures, but it does not scale well to large numbers of instances, and it does not behave well when clusters have very different sizes.

*** Gaussian Mixtures

A /Gaussian Mixture Model/ (GMM) is a generative model (new samples can be drawn from it) that assumes that the instances were generated from a mixture of several Gaussian distributions (we just don't know their parameters). It is possible to estimate the density of the model at any given location.

This method relies on the /Expectation-Maximization/ (EM) algorithm (which has many similarities with K-Means). EM first assigns instances to clusters (/expectation step/), then it updates the clusters (/maximization step/), and this process is repeated until convergence. In the context of clustering, EM can be thought of as a generalization of K-Means that, in addition to finding the clusters centers, can also find their size, shape and orientation, as well as their relative weights. Unlike K-Means, EM uses soft cluster assignments. EM is also sensitive to its initial condition, which can lead to poor solutions, so its best to run it multiple times.

EM struggles to converge to the optimal solution when there are many dimensions, many clusters, or few instances. We can alleviate this by limiting the number of parameters the algorithm has to learn. One way to achieve this is by imposing constraints on the covariance matrix (~covariance_type~ hyperparameter).

**** Anomaly Detection Using Gaussian Mixtures

Anomaly detection is the task of detecting instances that deviate strongly from the norm. These instances are called /anomalies/, or /outliers/, while the normal instances are called /inliers/. Any instance located in a low-density region can be considered an anomaly. If the density of a region is lesser than a given threshold, it is considered to be low-density.

Outlier detection is often used to clean up a dataset.

If there are too many outliers on a dataset, this will bias the model's view of what is "normal". If this happens, the model may be trained once to detect and remove the most extreme outliers, and then trained again on the cleaned-up dataset. Another option is to use robust covariance estimation methods (see the ~EllipticEnvelope~ class).

**** Selecting the Number of Clusters

Neither the inertia or the silhouette score are reliable when the clusters are not spherical or have different sizes, hence they are not very useful to select the appropriate number of clusters when using Gaussian mixtures. Instead, the model that minimizes a /theoritical information criterion/ such as the /Bayesian information criterion/ (BIC) or the /Akaike information criterion/ (AIC) can be used.

Both the BIC and the PIC penalize models that have more parameters to learn (e.g., more clusters), and reward models that fit the data well. They often end up selecting the same model. When they differ, the model selected by BIC tends to be simpler, but tends to not fit the data quite as well (especially true for larger datasets).

**** Bayesian Gaussian Mixture Models

The ~BayesianGaussianMixture~ class is capable of giving weights equal (or close) to zero to unnecessary clusters. Setting the number of clusters of the model to be greater than the actual number of clusters can lead to a model that eliminates the unnecessary clusters automatically.

**** Other Algorithms for Anomaly and Novelty Detection

*PCA*: The reconstruction error of an anomaly is usually much larger than the reconstruction error of a normal instance. This can be used as a simple an efficient anomaly detection approach.

*Fast-MCD*: This algorithm (implemented by the ~EllipticEnvelope~ class) assumes that the normal instances are generated from a single Gaussian distribution, and that the model is contaminated with outliers that were not generated from this Gaussian distribution. When the algorithm estimates the parameters of the distribution, it ignores the instances that are most likely outliers.

*Isolation Forest*: This algorithm builds a Random Forest in which the Decision Trees are grown randomly. The datasets gradually gets chopped into pieces, until all instances end up isolated from the other instances. Anomalies are usually far from the normal instances, so across all the Decision Trees they tend to get isolated in fewer steps.

*Local Outlier Factor* (LOF): Compares the density of instances around a given instance to the density around its neighbors. An anomaly tends to be more isolated than its $k$ nearest neighbors.

*One-Class SVM*: The one-class SVM algorithm tries to separate the instances in high-dimensional space from the origin (which correspond to finding a small region that encompasses all the instances in the original space). If a new instance does not fall within this region, it is an anomaly.

* Part II - Neural Networks and Deep Learning

** 10. Introduction to Artificial Neural Networks with Keras

The inspiration to build intelligent machines comes from the brain's architecture: /artificial neural networks/ (ANN) is a Machine Learning model inspired by the networks of biological neurons found in our brains. Anna are also the very core of Deep Learning!

*** From Biological to Artificial Neurons

Artificial Neural Networks have quite a lot going for them, hence the wave of interest, which is likely here to say this time. Here are a few good reasons for that:

- ANNs frequently outperform other ML algorithms on very large and complex problems.
- Due to the huge increase in computing power since the 1990s, it is now possible to train large neural networks in a reasonable amount of time.
- The training algorithms have improved to be better and more efficient.
- Some of the theoretical limitations of ANNs, which were a reason of concern, turned out to be benign in practice. For example, the training algorithms rarely get stuck in local optima.

**** Logical Computation with Neurons

A very simple model of the biological neuron, known as an /artificial neuron/, was proposed in the 40s by McCulloch and Pitts: it has one or more binary inputs and one binary output. The artificial neuron activates its output when more than a certain number of its inputs are active.

The original paper showed that even with such a simplified model, it is possible to build networks of artificial neurons capable of computing any logical proposition.

**** The Perceptron

One of the simplest ANN architectures was created by Frank Rosenblatt in 1957: the /Perceptron/. It is based on a different artificial neuron/ called a /threshold logic unit/ (TLU), or /linear threshold unit/ (LTU). Instead of binary values, the inputs are number, and each input connection is associated with a weight. The TLU computes a weighted sum of its inputs $z = \rm{x}^\top \rm{w}$, then applies a /step function/ to that sum and outputs the result: $h_{\rm{w}}(\rm{x}) = \text{step}(z)$

The most common step function used in Perceptrons is the /Heaviside step function/, but sometimes the sign function is used instead:

$$
\begin{align*}
\text{heaviside} (z) = \begin{cases}
    0 & \text{if}\ z < 0 \\
    1 & \text{if}\ z \ge 0
\end{cases} &&
\text{sign} (z) = \begin{cases}
    -1 & \text{if}\ z < 0 \\
    0 & \text{if}\ z = 0 \\
    1 & \text{if}\ z > 0
\end{cases}
\end{align*}
$$

A single TLU can be used for simple linear binary classification: if the linear combination of the inputs exceeds a threshold, it outputs the positive class, otherwise it outputs the negative class.

A Perceptron is composed of a single layer of TLUs, with each connected to all the inputs. The layer is called a /fully connected layer/ (or a /dense layer/) when all the neurons in it are connected to every neuron in the previous layer. The input neurons form the /input layer/. An extra bias feature is generally added, which is typically represented using a special type of neuron called a /bias neuron/.

Note: The name Perceptron is sometimes used to refer to a tiny network with a single TLU.

The Hebb's rule (or /Hebbian learning/) says that the connection weight between two neurons tends to increase when they fire simultaneously. Perceptrons are trained using a variant of this rule that takes into account the error made by the network and it makes a prediction, which reinforces connections that help reduce the error.

The decision boundary of each output neuron is linear, so Perceptrons are incapable of learning complex patterns, which renders them incapable of solving some trivial problems such as the /Exclusive OR/ (XOR) classification problem.

Scikit-Learn provides a ~Perceptron~ class that implements a single-TLU network. Note that Perceptrons make predictions based on a hard threshold.

Fortunately, we can eliminate some of the limitations of Perceptrons by stacking multiple of them. The resulting ANN is called a /Multilayer Perceptron/ (MLP).

**** The Multilayer Perceptron and Backpropagation

A MLP is composed of one input layer, one or more layer or TLU, called /hidden layers/, and on final layer of TLUs called the /output layer/. Every layer except the output layer includes a bias neuron and is fully connected to the next layer. A network in which the signal flows only from the inputs to the outputs is called a /feedforward neural network/ (FNN).

When an ANN contains a deep (the definition if "deep" is quite fuzzy) stack of hidden layers, it is called a /deep neural network/ (DNN).

The /backpropagation/ algorithm was introduced in 1986, and it is still used today to train MLPs. It is basically Gradient Descent with an efficient technique for computing the gradients automatically (this is called /automatic differentiation/, or /autodiff/): in just two passes through the network (one forward, one backward), the backpropagation algorithm is able to compute the gradient of the network's error with regard to every single parameter. With the gradients computed, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges.

Let's explore the algorithm in a bit more detail this time:

- It handles on mini-batch at a time, and it goes through the full training set multiple times. Each pass is called an /epoch/.
- Each mini-batch is passed to the input layer, which sends it to the first hidden layer, where the output of all neurons is computed for every instance in the mini-batch. The result is passed to the next layer, its output is computed and so on until we get to the output layer. This is what's called the /forward pass/.
- After that, the algorithm measures the network's output error using some loss function that compares the desired output and the actual output of the network.
- Next, it computes how much each output connection contributed to the error analytically by applying the /chain rule/.
- The algorithm measures how much of these error contributions came from each connection in the layer below (using the chain rule again), working backward until the input layer.
- Lastly, the algorithm performs a Gradient Descent step to tweak all the connection weights in the network.

In short: for each training instance, the backpropagation algorithm makes a prediction (forward pass) and measures the error, then goes through each layer in reverse to measure the error contribution from each connection (reverse pass), and finally tweaks the connection weights to reduce error with a Gradient Descent step.

Warning: It is important to initialize all hidden layers' connection weights randomly, or else training will fail.

A key change was made to the MLP architecture in order for this algorithm to work properly: the step function was replaced with the logistic (sigmoid) function. This was essential because the step function has no gradient to work with (it contains only flat segments, and Gradient Descent cannot move on a flat surface), while the logistic function has a well-defined nonzero derivative everywhere.

Note that the backpropagation algorithm works well with many other activation functions, not just the logistic function. These are two popular choices:

The /hyperbolic tangent/ function is also S-shaped, continuous and differentiable, but its output value ranges from -1 to 1. This range tends to make the layer's output more or less centered around 0 at the beginning of training, which often helps speed up convergence.

The Rectified Linear Unit function (ReLU) is continuous but not differentiable at $z = 0$. In practice, however, it works very well and is fast to compute, so it has become the default. The fact that it does not have a maximum output value helps reduce some issues during Gradient Descent.

**** Regression MLPs

MLPs can be used for regression tasks! To predict a single value, it needs just a single output neuron, which outputs the predicted value. For multivariate regression, one output neuron per output dimension is needed.

In general, not all activation function are appropriate for regression tasks: it's usually more appropriate to use one where the MLP is free to output any range of values. The ReLU can be used in the output layer to guarantee that the output will always be positive. Alternatively, /softplus/ can be used as well, which is a smooth variant of ReLU. To guarantee that the predictions will fall within a given range of value, the logistic function (or the hyperbolic tangent) can be used and then scaled to the appropriate ranges.

The loos function used during training is typically the mean squared error, but the mean absolute error might be preferred instead if there are lots of outliers in the training set. The Huber loss, which is a combination of both, is a good option as well (it makes the MLP less sensitive to outliers than the MSE, and converges faster than the mean absolute error).

The following table summarizes the typical architecture of a regression MLP:

| *Hyperparameter*           | *Typical value*                                                                    |
|----------------------------+------------------------------------------------------------------------------------|
| # input neurons            | One per input feature                                                              |
| # hidden layers            | Depends on the problem (typically 1 to 5)                                          |
| # neurons per hidden layer | Depends on the problem (typically 10 to 100)                                       |
| # output neurons           | 1 per prediction dimension                                                         |
| Hidden activation          | ReLU (or SELU)                                                                     |
| Output Activation          | None, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs) |
| Loss function              | MSE or MAE/Huber (if outliers)                                                     |


**** Classification MLPs

MLPs can be used for classification tasks as well! For a binary classification problem, we just nee da single output neuron using the logistic activation function, which will output a number that can be interpreted as the estimated probability of the positive class.

For multilabel binary classification tasks, there would be one output neuron dedicated to each positive class.

If the classes are exclusive (for example in the MNIST dataset), the MLP would need to have one output neuron per class, and the softmax activation function should be used for the whole output layer to ensure that all th estimated probabilities are between 0 and 1 add that they up to 1.

For the loss function, the cross-entropy loss is generally a good choice, since we are predicting probability distributions.

The following table summarizes nicely the typical architecture of a classification MLP:

| *Hyperparameter*        | *Binary classification* | *Multilabel binary classification* | *Multiclass classification* |
|-------------------------+-------------------------+------------------------------------+-----------------------------|
| Input and hidden layers | Same as regression      | Same as regression                 | Same as regression          |
| # output neurons        | 1                       | 1 per label                        | 1 per class                 |
| Output layer activation | Logistic                | Logistic                           | Softmax                     |
| Loss function           | Cross entropy           | Cross entropy                      | Cross entropy               |

*** Implementing MLPs

When dealing with sparse labels (e.g., just a target class index), the ~sparse_categorical_crossentropy~ should be used if the class are exclusive. If instead there is a probability per class for each instance (e.g., one-hot encoding), than we should use the ~categorical_crossentropy~ instead.

If the problem is a binary classification problem, the ~sigmoid~ activation function should be used with the ~binary_crossentropy~ loss function.

Tip: When plotting the training curve, it should be shifted by half an epoch to the left.

Remember to resist the temptation to tweak the hyperparameters based on the test set performance, or else the estimate of the generalization will be too optimistic (and most likely unrealistic).

**** Complex Models

One example of a nonsequential neural network is the /Wide & Deep/ neural network: it connects all parts of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path), and simple rules (using the short path). A regular MLP forces all the data to flow through the full stack of layers, which might result in simple patterns in the data end up being distorted by this sequence of transformations.

*** Fine-Tuning Neural Network Hyperparameters

The flexibility of neural networks is also one of their main drawback: there are many hyperparameters to tweak.

Optimizing hyperparameters is usually a difficult and time-consuming task. Here are some Python libraries that can help with this process:

- *Hyperopt*
- *Hyperas*, *kopt*, or *Talos*
- *Keras Tuner*
- *Scikit-Optimize (skopt)*
- *Spearmint*
- *Hyperband*
- *Sklearn-Deap*

**** Number of Hidden Layers

For many problems we can start with just one or two hidden layers and the neural network will work just fine. For more complex problems, we can ramp up the number of hidden layers until the model starts to overfit the training set.

Instead of randomly initializing the weights and biases of the first few layers of a new neural network, we can initialize them to the values of the weights and biases of the lower layers of a network that has already been trained to solve a similar problem. This way the new network will not have to learn all the low-level structures from scratch: it will only have to learn the higher-level structures problem to the specific problem we are trying to solve. This technique is called /transfer learning/.

We rarely have to train neural networks from scratch: it is much more common to reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be much faster and require much less data.

**** Number of Neurons per Hidden Layer

The number of neurons in the input and output layers is determined by the type of input and output the task requires.

As for the hidden layers, it used to be common to size them to form a pyramid, with fewer and fewer neurons at each layer. This practice has been abandoned because it seems that using the same number of neurons in all hidden layers perform just as well in most cases, if not better. That said, depending on the dataset, it can be helpful to make the first hidden layer bigger than the others.

Just like the number of hidden layers, the number of neurons can be gradually increased until the network starts overfitting. However, in practice is often simpler and more efficient to pick a model with more layers and neurons that actually needed, then use early stop and other regularization techniques to prevent it from overfitting.

Tip: In general, it is better to increase the number of layers instead of the number of neurons per layer.

**** Hyperparameters

Let's briefly talk about the most important hyperparameters, as well as tips on how to set them:

*Learning rate*: This is arguably the most important hyperparameter. The optimal learning rate is about half the maximum learning rate (the learning rate above which the training algorithm diverges). One way to find a good learning rate is to train a model starting with a very low learning rate, and gradually increase it up to a very large value. In the plot of the loss as a function of the learning rate, the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than that point).

*Optimizer*: Using a better optimized than Mini-batch Gradient Descent is also quite important. This will be discussed in more details in the next chapter.

*Batch size*: The batch size can have a significant impact on a model's performance and training time. Using large batch sizes has the benefit that hardware accelerators like GPUs can process them more efficiently. This is why many researches recommend using the largest batch size that can fit in GPU RAM. However, it's not that simple: large batch sizes may lead to training instabilities, especially at the beginning of the training, and the resulting model may not be able to generalize as well as a model trained with a small batch size. With this in mind, one strategy is to use a large batch size, using learning rate warmup, and if training is unstable or the performance is disappointing, try using a smaller batch size instead.

*Activation function*: In general, the ReLU activation function is a good default for all hidden layers. For the output layer, it depends on the task at hand.

*Number of iterations*: In most cases this parameter does not need to be tweaked: just use early stop instead.

Tip: The optimal learning rate depends on the other hyperparameters (especially the batch size), so if the hyperparameters are modified, the learning rate must be updated as well.

For more best practices regarding tuning neural network hyperparameters, check out this [[https://arxiv.org/abs/1803.09820][excellent paper]] by Leslie Smith.
