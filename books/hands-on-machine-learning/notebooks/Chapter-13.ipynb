{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Dealing with large datasets and preprocessing them efficiently can be challenging. The Data API is a tool that makes this fairly simple with the many features it offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "Splitting a large dataset into many files has many benefits:\n",
    "- It makes it possible to shuffle the dataset at a coarse level before shuffling it at a finer level.\n",
    "- It simpler to manipulate many smaller files rather than a huge file\n",
    "- If the data is split across multiple servers across multiple devices, it is possible to download several files from different servers simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "If the GPU is not being fully utilized, it is possible that the input pipeline is the bottleneck. This may be fixed by reading and preprocessing the data in multiple threads in parallel, and prefetching a few batches. A properly optimized preprocessing function can also help a lot. Saving the dataset into multiple TFRecord files and performing some of the preprocessing ahead of time might be a good idea as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "Any binary data can be stored in a TFRecord file. In practice, most TFRecord files contain sequences of serialized protocol buffers, which allows them to be easily read across multiple platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "TensorFlow provides some handy operations to parse the `Example` protobuf format, which is flexible enough to represent instances in most datasets. If this is not the case, a custom protocol buffer can be defined for a specific application, but doing so requires deploying the descriptor along with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "Using compression is great to save space and bandwidth, at the cost of wasting CPU to decompress it. It really depends on the application and which resources are most valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "- Preprocessing the data when creating the data files will speed up the training process since no preprocessing on the fly will be required. If the data contains a lot of noise that is going to be filtered out, some disk space will be saved as well. However, this approach will limit the flexibility of experimenting with various preprocessing pipelines. Moreover, the trained model will expect preprocessed data, which will add a layer of complexity to the deployed application (code to preprocess the data before feeding it to the model).\n",
    "\n",
    "- Using the `tf.data` pipeline to preprocess data will make it much easier to tweak the preprocessing logic, and it makes it easy to create highly efficient preprocessing pipelines. However, this approach will slow down training, and each training instance will be preprocessed once per epoch rather than just once when preparing the data beforehand (as with the previous approach). Lastly, the trained model will still expect preprocessed data.\n",
    "\n",
    "- Adding preprocessing layers to the model has the advantage that preprocessing code has to be written only once for both training and inference. The downside is that it will also slow down training, and each instance will be preprocessed once per epoch. Moreover, by default, these operations will be run on the GPU, but the upcoming Keras preprocessing layers should be able to benefit from multithreaded execution on the CPU.\n",
    "\n",
    "- Using TF Transform gives many of the benefits from the previous options: each instance is preprocessed just once (which speeds up training), and preprocessing layers get automatically generates so the preprocessing code is only written once. The main drawback is the learning curve required to use this tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "If the categorical feature has a natural order, a simple option is to use ordinal encoding. If it does not have such a natural order, one-hot encoding can be used, or embeddings if there are many categories.\n",
    "\n",
    "One option to encode text is the bag-of-words representation. However, it can favor words that are usually not very important, so TF-IDF is a popular option to reduce their weight. Also, instead of counting the only words, it is common to count _n_-grams. Using word embeddings to encode the text is also a viable option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, y_train = X_train_full[5000:], y_train_full[5000:]\n",
    "X_val, y_val = X_train_full[:5000], y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
    "val_set = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example\n",
    "\n",
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    \n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                'image': Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                'label': Feature(int64_list=Int64List(value=[label]))\n",
    "            }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def save_tfrecords(name, data, n_records=20):\n",
    "    filepaths = [f\"{name}_{idx:02d}.tfrecord\" for idx in range(n_records)]\n",
    "    \n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path)) for path in filepaths]\n",
    "        \n",
    "        for idx, (image, label) in data.enumerate():\n",
    "            file_idx = idx % n_records\n",
    "            writers[file_idx].write(create_example(image, label).SerializeToString())\n",
    "            \n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = save_tfrecords('fashion_mnist-train', train_set)\n",
    "val_filepaths = save_tfrecords('fashion_mnist-val', val_set)\n",
    "test_filepaths = save_tfrecords('fashion_mnist-test', test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    \n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example['image'], out_type=tf.uint8)\n",
    "    \n",
    "    return tf.reshape(image, shape=[28, 28]), example['label']\n",
    "\n",
    "def load_dataset(filepaths, batch_size=32, shuffle_buffer_size=None, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths, num_parallel_reads=os.cpu_count())\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=os.cpu_count())\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = load_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "val_set = load_dataset(val_filepaths)\n",
    "test_set = load_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "    \n",
    "standardization_layer = Standardization(input_shape=[28, 28])\n",
    "\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()), axis=0).astype(np.float32)\n",
    "\n",
    "standardization_layer.adapt(sample_images)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    standardization_layer,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 4s 2ms/step - loss: 640.4513 - accuracy: 0.8403 - val_loss: 401.2222 - val_accuracy: 0.8664\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 765.3571 - accuracy: 0.8771 - val_loss: 449.0884 - val_accuracy: 0.8692\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 465.4117 - accuracy: 0.8911 - val_loss: 1157.0330 - val_accuracy: 0.8796\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 319.1557 - accuracy: 0.8992 - val_loss: 2328.1399 - val_accuracy: 0.8690\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 263.9018 - accuracy: 0.9073 - val_loss: 1001.7927 - val_accuracy: 0.8852\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, epochs=5, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/tmp/.keras/datasets/aclImdb')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "download_path = keras.utils.get_file(filename, url, extract=True, cache_dir='/tmp/.keras/')\n",
    "path = Path(download_path).parent / 'aclImdb'\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_filepaths(dirpath):\n",
    "    return np.array([str(path) for path in dirpath.glob('*.txt')])\n",
    "\n",
    "train_pos = data_filepaths(path / 'train' / 'pos') \n",
    "train_neg = data_filepaths(path / 'train' / 'neg')\n",
    "test_val_pos = data_filepaths(path / 'test' / 'pos')\n",
    "test_val_neg = data_filepaths(path / 'test' / 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.RandomState(42).permutation(len(test_val_pos))\n",
    "test_val_pos = test_val_pos[perm]\n",
    "test_val_neg = test_val_neg[perm]\n",
    "\n",
    "test_pos = test_val_pos[:5000]\n",
    "test_neg = test_val_neg[:5000]\n",
    "val_pos = test_val_pos[5000:]\n",
    "val_neg = test_val_neg[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filepaths_pos, filepaths_neg):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for filepaths, label in ((filepaths_pos, 1), (filepaths_neg, 0)):\n",
    "        for filepath in filepaths:\n",
    "            with open(filepath) as review:\n",
    "                reviews.append(review.read())\n",
    "            labels.append(label)\n",
    "    return tf.data.Dataset.from_tensor_slices((tf.constant(reviews), tf.constant(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = create_dataset(train_pos, train_neg).shuffle(len(train_pos) + len(train_neg)).batch(batch_size).prefetch(1)\n",
    "val_set = create_dataset(val_pos, val_neg).batch(batch_size).prefetch(1)\n",
    "test_set = create_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(X_batch, n_words=50):\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, 50]) \n",
    "    Z = tf.strings.substr(X_batch, 0, 300)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b'<br\\\\s*/?>', b' ')\n",
    "    Z = tf.strings.regex_replace(Z, b'[^a-z]', b' ')\n",
    "    Z = tf.strings.split(Z)\n",
    "    return Z.to_tensor(shape=shape, default_value=b'<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 2, 50], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
    "tf.shape(X_example) * tf.constant([1, 0]) + tf.constant([0, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size=1000):\n",
    "    preprocessed_reviews = preprocess_text(data_sample).numpy()\n",
    "    counter = Counter()\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b'<pad>':\n",
    "                counter[word] += 1\n",
    "    return [b'<pad>'] + [word for word, count in counter.most_common(max_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets = n_oov_buckets\n",
    "        \n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
    "        words = tf.constant(self.vocab)\n",
    "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess_text(inputs)\n",
    "        return self.table.lookup(preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 50), dtype=int64, numpy=\n",
       "array([[ 1,  3,  4,  2,  2,  5,  6,  7,  1,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0],\n",
       "       [ 1,  8,  9, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0]])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "text_vectorization = TextVectorization()\n",
    "\n",
    "text_vectorization.adapt(X_example)\n",
    "text_vectorization(X_example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()), axis=0)\n",
    "\n",
    "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets, input_shape=[])\n",
    "\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 token for <pad>\n",
    "bag_of_words = BagOfWords(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    bag_of_words,\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.5447 - accuracy: 0.7140 - val_loss: 0.5125 - val_accuracy: 0.7396\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 5s 7ms/step - loss: 0.4683 - accuracy: 0.7723 - val_loss: 0.5056 - val_accuracy: 0.7449\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.4173 - accuracy: 0.8080 - val_loss: 0.5142 - val_accuracy: 0.7477\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.3460 - accuracy: 0.8534 - val_loss: 0.5300 - val_accuracy: 0.7418\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 0.2644 - accuracy: 0.9010 - val_loss: 0.5692 - val_accuracy: 0.7347\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, epochs=5, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_mean(inputs, axis=1) * sqrt_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    keras.layers.Embedding(input_dim=n_tokens, output_dim=embedding_size, mask_zero=True),\n",
    "    keras.layers.Lambda(mean_embedding),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5547 - accuracy: 0.7096 - val_loss: 0.5139 - val_accuracy: 0.7394\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4935 - accuracy: 0.7554 - val_loss: 0.5072 - val_accuracy: 0.7440\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4818 - accuracy: 0.7584 - val_loss: 0.5099 - val_accuracy: 0.7426\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4730 - accuracy: 0.7626 - val_loss: 0.5104 - val_accuracy: 0.7427\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.4664 - accuracy: 0.7658 - val_loss: 0.5108 - val_accuracy: 0.7421\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "history = model.fit(train_set, epochs=5, validation_data=val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name='imdb_reviews')\n",
    "train_set, test_set = datasets['train'], datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"Oh yeah! Jenna Jameson did it again! Yeah Baby! This movie rocks. It was one of the 1st movies i saw of her. And i have to say i feel in love with her, she was great in this move.<br /><br />Her performance was outstanding and what i liked the most was the scenery and the wardrobe it was amazing you can tell that they put a lot into the movie the girls cloth were amazing.<br /><br />I hope this comment helps and u can buy the movie, the storyline is awesome is very unique and i'm sure u are going to like it. Jenna amazed us once more and no wonder the movie won so many awards. Her make-up and wardrobe is very very sexy and the girls on girls scene is amazing. specially the one where she looks like an angel. It's a must see and i hope u share my interests\", shape=(), dtype=string)\n",
      "tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for example in train_set.take(1):\n",
    "    print(example['text'])\n",
    "    print(example['label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:handson-tf2]",
   "language": "python",
   "name": "conda-env-handson-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
