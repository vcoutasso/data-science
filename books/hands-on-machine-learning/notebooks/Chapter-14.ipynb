{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "The CNN will automatically learn to extract features from the input image; a CNN has many fewer parameters than a fully connected DNN, which makes it faster to train, and less prone to overfitting; the kernel of a CNN that can detect a particular feature can be used to detect that feature anywhere in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "- Use 16-bit floats instead of 32-bit.\n",
    "- Reduce the mini-batch size.\n",
    "- Reduce dimensionality using a larger stride.\n",
    "- Remove some layers.\n",
    "- Distribute the CNN across multiple devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "A max pooling layer has the advantage of having no parameters at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "A local response normalization layer encourages different feature maps to specialize, and pushes them apart, forcing them to explore a wider range of features. It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "AlexNet is much wider and deeper, and it stacks convolutional layers directly on top of each other. GoogLeNet introduced inception modules, allowing for much deeper networks with fewer parameters. ResNet introduced skip connections. SENet introduced the idea of using an SE blocks to recalibrate the relative importance of feature maps. Xception introduced the use of depthwise separable convolutional layers, which look at spatial patterns and depthwise patterns separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "A fully convolutional network is a neural network composed exclusively of convolutional and pooling layers. To convert dense layers to convolutional layers, replace the lowest dense layer with a convolutional layer with a kernel size equal to the layer's input size, with one filter per neuron in the dense layer, and using `\"valid\"` padding. The stride should generally be 1, and the activation function should be the same. The other layers should be converted the same way, but using $1 \\times 1$ filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "The main problem is that much of the spatial information gets lost in a CNN as the signal flows through the network, especially in pooling layers and layers with a stride greater than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255.\n",
    "\n",
    "X_train, X_val = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_val = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_train, X_val, X_test = X_train[..., np.newaxis], X_val[..., np.newaxis], X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(16, kernel_size=5, padding='same', activation='relu'),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding='same', activation='relu'),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "215/215 [==============================] - 27s 127ms/step - loss: 0.3624 - accuracy: 0.8910 - val_loss: 0.0668 - val_accuracy: 0.9824\n",
      "Epoch 2/10\n",
      "215/215 [==============================] - 27s 123ms/step - loss: 0.1033 - accuracy: 0.9697 - val_loss: 0.0492 - val_accuracy: 0.9868\n",
      "Epoch 3/10\n",
      "215/215 [==============================] - 28s 131ms/step - loss: 0.0735 - accuracy: 0.9777 - val_loss: 0.0411 - val_accuracy: 0.9880\n",
      "Epoch 4/10\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.0592 - accuracy: 0.9818 - val_loss: 0.0403 - val_accuracy: 0.9890\n",
      "Epoch 5/10\n",
      "215/215 [==============================] - 26s 121ms/step - loss: 0.0500 - accuracy: 0.9845 - val_loss: 0.0396 - val_accuracy: 0.9902\n",
      "Epoch 6/10\n",
      "215/215 [==============================] - 25s 119ms/step - loss: 0.0468 - accuracy: 0.9863 - val_loss: 0.0367 - val_accuracy: 0.9906\n",
      "Epoch 7/10\n",
      "215/215 [==============================] - 25s 118ms/step - loss: 0.0357 - accuracy: 0.9886 - val_loss: 0.0353 - val_accuracy: 0.9910\n",
      "Epoch 8/10\n",
      "215/215 [==============================] - 26s 119ms/step - loss: 0.0325 - accuracy: 0.9898 - val_loss: 0.0307 - val_accuracy: 0.9906\n",
      "Epoch 9/10\n",
      "215/215 [==============================] - 26s 119ms/step - loss: 0.0299 - accuracy: 0.9907 - val_loss: 0.0322 - val_accuracy: 0.9916\n",
      "Epoch 10/10\n",
      "215/215 [==============================] - 27s 124ms/step - loss: 0.0278 - accuracy: 0.9911 - val_loss: 0.0349 - val_accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0281 - accuracy: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.02811438962817192, 0.9900000095367432]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    preprocessed_image =  keras.applications.mobilenet_v2.preprocess_input(resized_image)\n",
    "    \n",
    "    return preprocessed_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set, test_set = tfds.load(\n",
    "    'tf_flowers',\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True,\n",
    "    split=[\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[:80]),\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[80:90]),\n",
    "        tfds.Split.TRAIN.subsplit(tfds.percent[90:]),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "val_set = val_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(10, activation='softmax')(avg)\n",
    "\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "93/93 [==============================] - 66s 712ms/step - loss: 3.1696 - accuracy: 0.7912 - val_loss: 1.8300 - val_accuracy: 0.8583\n",
      "Epoch 2/3\n",
      "93/93 [==============================] - 69s 739ms/step - loss: 0.9702 - accuracy: 0.9166 - val_loss: 0.9844 - val_accuracy: 0.9000\n",
      "Epoch 3/3\n",
      "93/93 [==============================] - 70s 753ms/step - loss: 0.4721 - accuracy: 0.9420 - val_loss: 0.9224 - val_accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "optimizer = keras.optimizers.SGD(lr=0.15, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_set, validation_data=val_set, epochs=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "93/93 [==============================] - 325s 3s/step - loss: 1.7072 - accuracy: 0.7654 - val_loss: 35.8445 - val_accuracy: 0.4278\n",
      "Epoch 2/5\n",
      "93/93 [==============================] - 323s 3s/step - loss: 0.3355 - accuracy: 0.8905 - val_loss: 21.1546 - val_accuracy: 0.5722\n",
      "Epoch 3/5\n",
      "93/93 [==============================] - 314s 3s/step - loss: 0.0945 - accuracy: 0.9712 - val_loss: 16.2478 - val_accuracy: 0.5917\n",
      "Epoch 4/5\n",
      "93/93 [==============================] - 284s 3s/step - loss: 0.0539 - accuracy: 0.9837 - val_loss: 10.0330 - val_accuracy: 0.6444\n",
      "Epoch 5/5\n",
      "93/93 [==============================] - 291s 3s/step - loss: 0.0164 - accuracy: 0.9973 - val_loss: 7.2679 - val_accuracy: 0.6861\n"
     ]
    }
   ],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n",
    "                                 nesterov=True, decay=0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, validation_data=val_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 7s 558ms/step - loss: 6.7751 - accuracy: 0.6806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6.775132179260254, 0.6805555820465088]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:handson-tf2]",
   "language": "python",
   "name": "conda-env-handson-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
