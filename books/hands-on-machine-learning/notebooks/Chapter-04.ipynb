{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. \n",
    "\n",
    "Stochastic GD and Mini-batch GD are probably the most appropriate options for datasets with too many features. Batch GD can also be a good option if out-of-core training is not strictly necessary (i.e., the training set fits in memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "Different scales can affect the convergence speed of Gradient Descent, since the shape of the cost function would be much like an elongated bowl. Regularized models can also be affected and may converge to a suboptimal solution: since regularization penalizes large wights, features with smaller values might be ignored compared to features with larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "No, because the cost function of a Logistic Regression model is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "No, because Stochastic GD and Mini-batch GD will never really converge. However, for convex problems and a learning rate not too high, all Gradient Descent algorithms will approach the global optimum, yielding very similar models (and even then the models are different still)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "If only the validation error going up, it is likely that the model is overfitting the training data, and the training should be stopped. If both errors are going up, the model is diverging due to a learning rate too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "No, because Mini-batch Gradient Descent is not guaranteed to make progress at every single training step. Due to the nature of the algorithm, the cost function can bounce around a little, going up and down, and still be improving on average. If the model has not improved for a long time, however, it is more likely that it has already peaked and training should be stopped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "Stochastic GD has the fastest training time, so it will *probably* be the first to reach the vicinity of the global optimum. However, only Batch Gradient Descent will actually converge, while both Stochastic GD and Mini-batch GD will bounce around the optimum (unless the training rate is gradually reduced during training)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "The large gap probably indicates that the model is not generalizing very well for some reason, or even overfitting the training data. Some approaches to solve this are:\n",
    "- Maybe the model is too complex (i.e., has too many degrees of freedom), so reducing the polynomial degree might help;\n",
    "- Regularization of the model is also a viable option to reduce the complexity of the model;\n",
    "- Increasing the size of the training set may help with the generalization issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "In this situation, the model is likely suffering from high bias, due to the assumption that the data is less complex than it intrinsically is, which leads to underfitting. The regularization parameter $\\alpha$ should be reduced to help with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "\n",
    "1. Generally, a model with some regularization tends to perform better than a model without any. Ridge Regression is a good default and should be preferred over plain Linear Regression.\n",
    "2. Lasso encourages sparse models by means of the $\\ell_1$ penalty. This can help with automatic feature selection (if we have a reason to suspect that only a few of them matter) and even noise filtering. If we have no reason to believe this, Ridge Regression should be preferred.\n",
    "3. Elastic Net offers a good compromise between both regularization strategies. This is especially appealing since Lasso may behave erratically in some cases (when several features are strongly correlated or when there are more features than training instances). The main disadvantage is that it add an additional hyperparameter to tune. For Lasso without the erratic behavior, Elastic Net can be used with an `l1_ratio` close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "Two Logistic Regression classifiers would be more appropriate, since these classes are not exclusive (i.e., all four combinations are possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.\n",
    "This exercise requires us to implement Batch Gradient Descent (with early stopping) for Softmax Regression, without using scikit-learn, but I decided to use some helper functions in order to easily demonstrate the implementation working.\n",
    "\n",
    "We need to implement the cost function and gradient equations. The cross entropy cost function is given by\n",
    "\n",
    "$$\n",
    "J(\\rm{\\Theta}) = -\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}y_{k}^{(i)}\\log\\left(\\hat{p}_{k}^{(i)}\\right)\n",
    "$$\n",
    "\n",
    "and the cross entropy gradient vector for class $k$ is given by\n",
    "\n",
    "$$\n",
    "\\nabla_{\\rm{\\theta}^{(k)}}J(\\rm{\\Theta}) = \\frac{1}{m}\\sum_{i=1}^{m}\\left(\\hat{p}_{k}^{(i)} - \\hat{y}_{k}^{(i)} \\right)x^{(i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Batch Gradient Descent with early stopping for Softmax Regression (aka Multinomial Logistic Regression)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def softmax_regression_fit(X_train, y_train, X_val, y_val, penalty=None, alpha=0.1, max_iter=5001, lr=0.01, verbose=False):\n",
    "    \"\"\"\n",
    "    Fit a Softmax Regression model with early stopping and optional regularization\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : array-like of shape (n_samples, n_features)\n",
    "        Training data\n",
    "    y_train : array-like of shape (n_samples, n_classes)\n",
    "        One-hot-encoded training labels\n",
    "    X_val : array-like of shape (n_samples, n_features)\n",
    "        Validation data\n",
    "    y_val : array-like of shape (n_samples, n_classes)\n",
    "        One-hot-encoded validation labels\n",
    "    penalty : {'l1', 'l2', None}, default=None\n",
    "        Regularization method\n",
    "    alpha : float, default=0.1\n",
    "        Regularization hyperparameter\n",
    "    max_iter : int, default=1000\n",
    "        Maximum number of iterations\n",
    "    lr : float, default=0.01\n",
    "        Learning rate\n",
    "    verbose : bool, default=False\n",
    "        Be verbose\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Theta : array-like of shape (n_features + 1, n_classes)\n",
    "        Model parameter matrix\n",
    "    \"\"\"\n",
    "    # Add the bias term for every instance\n",
    "    X_bias = np.c_[np.ones(len(X_train)), X_train]\n",
    "    X_bias_val = np.c_[np.ones(len(X_val)), X_val]\n",
    "    \n",
    "    # Number of instances\n",
    "    m = X_bias.shape[0]\n",
    "    # Number of features + bias term\n",
    "    n_inputs = X_bias.shape[1] \n",
    "    # Number of classes\n",
    "    n_outputs = y_train.shape[1]\n",
    "    \n",
    "    # Add tiny value to compute the log of the probability of instance i belonging to class x,\n",
    "    # in order to avoid nan values when this probability is 0\n",
    "    epsilon = 1e-9\n",
    "    \n",
    "    # Randomly initialize parameter matrix Theta. Each column corresponds to the parameter vector of a given class\n",
    "    Theta = np.random.randn(n_inputs, n_outputs)\n",
    "    \n",
    "    best_loss = np.infty\n",
    "    \n",
    "    if penalty not in ['l1', 'l2']:\n",
    "        alpha = 0\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # Compute class scores\n",
    "        logits = X_bias.dot(Theta)\n",
    "        # Estimate the probabilites of each instance belonging to each class \n",
    "        proba = softmax(logits)\n",
    "        # Compute the loss function\n",
    "        loss = -np.mean(np.sum(y_train * np.log(proba + epsilon), axis=1))\n",
    "        # Add the regularization term\n",
    "        if penalty == 'l1':\n",
    "            loss += alpha * np.sum(np.abs(Theta[1:]))\n",
    "        elif penalty == 'l2':\n",
    "            loss += alpha * 0.5 * np.sum(np.square(Theta[1:]))\n",
    "        # Compute the gradient vectors\n",
    "        gradients = 1/m * X_bias.T.dot(proba - y_train) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "        # Update parameter matrix\n",
    "        Theta -= lr * gradients\n",
    "        \n",
    "        # Measure the loss on the validation set to verify if we should stop early\n",
    "        logits = X_bias_val.dot(Theta)\n",
    "        proba = softmax(logits)\n",
    "        loss = -np.mean(np.sum(y_val * np.log(proba + epsilon), axis=1))\n",
    "        if penalty == 'l1':\n",
    "            loss += alpha * np.sum(np.abs(Theta[1:]))\n",
    "        elif penalty == 'l2':\n",
    "            loss += alpha * 0.5 * np.sum(np.square(Theta[1:]))\n",
    "            \n",
    "        if verbose and iteration % 1000 == 0:\n",
    "            print(f\"Loss at iteration {iteration}: {loss:.4f}\")\n",
    "        \n",
    "        if loss < best_loss: \n",
    "            best_loss = loss\n",
    "        else:\n",
    "            if verbose: print(f\"Early stopping at iteration {iteration} with loss {loss}\")\n",
    "            break\n",
    "    \n",
    "    return Theta\n",
    "\n",
    "\n",
    "def softmax_regression_predict(Theta, X):\n",
    "    \"\"\"\n",
    "    Softmax Regression of the data X with parameters Theta\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Theta : array-like of shape (n_features + 1, n_classes)\n",
    "        Model parameter matrix\n",
    "    X : array-like of shape (n_samples, n_features)\n",
    "        Input data\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : array-like of shape (n_samples, n_classes)\n",
    "        Predicted labels of X\n",
    "    \"\"\"\n",
    "    X_bias = np.c_[np.ones(len(X)), X]\n",
    "    \n",
    "    logits = X_bias.dot(Theta)\n",
    "    proba = softmax(logits)\n",
    "    \n",
    "    return np.argmax(proba, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented it, we need to test it. We will use the iris dataset for that, with only two features. Let's load it and split into the train, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# Use only petal length and width\n",
    "X = X[:, (2, 3)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "y_train = OneHotEncoder(sparse=False).fit_transform(y_train[:, np.newaxis])\n",
    "y_val = OneHotEncoder(sparse=False).fit_transform(y_val[:, np.newaxis])\n",
    "y_test = OneHotEncoder(sparse=False).fit_transform(y_test[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model and retrieve its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 4.6653\n",
      "Loss at iteration 1000: 0.5456\n",
      "Loss at iteration 2000: 0.5364\n",
      "Loss at iteration 3000: 0.5348\n",
      "Loss at iteration 4000: 0.5345\n",
      "Loss at iteration 5000: 0.5343\n",
      "Loss at iteration 6000: 0.5343\n",
      "Loss at iteration 7000: 0.5343\n",
      "Loss at iteration 8000: 0.5343\n",
      "Loss at iteration 9000: 0.5343\n",
      "Loss at iteration 10000: 0.5343\n",
      "Loss at iteration 11000: 0.5343\n",
      "Loss at iteration 12000: 0.5343\n",
      "Loss at iteration 13000: 0.5343\n",
      "Loss at iteration 14000: 0.5343\n",
      "Loss at iteration 15000: 0.5343\n",
      "Loss at iteration 16000: 0.5343\n",
      "Loss at iteration 17000: 0.5343\n",
      "Loss at iteration 18000: 0.5343\n",
      "Loss at iteration 19000: 0.5343\n",
      "Loss at iteration 20000: 0.5343\n",
      "Early stopping at iteration 20444 with loss 0.5343070427809481\n"
     ]
    }
   ],
   "source": [
    "Theta = softmax_regression_fit(X_train, y_train, X_val, y_val, penalty='l2', lr=0.15, alpha=0.1, max_iter=50000, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, it even stopped earlier! Let's take a look at how it performs on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666666"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.argmax(y_val, axis=1)\n",
    "y_pred = softmax_regression_predict(Theta, X_val)\n",
    "np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over 90% accuracy! How about the test data? Let's see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.argmax(y_test, axis=1)\n",
    "y_pred = softmax_regression_predict(Theta, X_test)\n",
    "np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, it performed even better on the test set! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:handson-ml2]",
   "language": "python",
   "name": "conda-env-handson-ml2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
