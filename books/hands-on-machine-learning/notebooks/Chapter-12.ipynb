{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Models and Training with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "TensorFlow is a powerful library for numerical computations, particularly well suited for large-scale Machine Learning. It is similar to NumPy at its core, but with GPU support. Its main features include support for distributed computing, computation graph analysis and optimization, an optimization API based on reverse-mode autodiff, and several powerful APIs. Other popular Deep Learning libraries include Theano, Caffe and PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "TensorFlow offers most of the functionalities provided by NumPy, but it is not a drop-in replacement: the functions are not always named the same, some functions do not behave in the same way, and NumPy arrays are mutable, while TensorFlow tensors are not (although mutable objects can be created with `tf.Variable`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\n",
    "The tensor created by `tf.range(10)` uses 32-bit integer (TensorFlow defaults to 32 bits), while `tf.constant(np.arange(10))` will create a tensor containing 64-bit integers (which is NumPy's default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\n",
    "1. Ragged tensors\n",
    "2. Sparse tensors\n",
    "3. Sets\n",
    "4. Queues\n",
    "5. String tensors\n",
    "6. Tensor arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\n",
    "Subclassing the `keras.losses.Loss` class allows us to use custom loss function hyperparameters, and save them along with the model by implementing the `get_config()` method. When this is not the case, a regular Python function will suffice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\n",
    "Custom metrics can also be defined as regular Python functions. However, in order for a custom metric to support hyperparameters, it should subclass the `keras.metrics.Metric` class, which gives more flexibility and control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.\n",
    "The internal components of the model should be distinguished from the model itself. To achieve this, the former should subclass the `keras.layers.Layer` class, while the latter should subclass the `keras.models.Model` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.\n",
    "This should be done only when its strictly necessary to have full control over the training loop, as writing a custom training loop is more error-prone. In most cases, there are more than enough tools to customize the training without having to write a custom training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.\n",
    "Custom Keras components should be convertible to TF Function, meaning that they should stick to TF operations as much as possible, and respect the rules discussed in the book. If there is no way around it, arbitrary Python can be included, but this will reduce performance and limit the model's portability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.\n",
    "Refer the \"TF Function Rules\" section of the book for the complete list ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "A dynamic Keras model can be created by setting `dynamic=True` when creating it, or by setting `run_eagerly=True` when calling the model's `compile()` method. A dynamic model can be useful for debugging, since it will not compile any custom component to a TF Function. However, making a model dynamic prevents Keras from using TensorFlow's graph features, so it will slow down training and inference, and limit the model's portability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=6.700959e-08>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self, eps=0.001, **kwargs):\n",
    "        self.eps = eps\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(name='alpha', shape=input_shape[-1:], initializer='ones', dtype='float32')\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:], initializer='zeros', dtype='float32')\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return {**config, 'eps': self.eps}\n",
    "    \n",
    "# Generate random data\n",
    "X = np.random.rand(1000, 2).astype(np.float32)\n",
    "\n",
    "custom_layer = LayerNormalization()\n",
    "keras_layer = keras.layers.LayerNormalization()\n",
    "\n",
    "# Compute the MAE for the output of the two layers.\n",
    "# We can verify that the outputs are similar since the error is very low\n",
    "tf.reduce_mean(keras.losses.mean_absolute_error(keras_layer(X), custom_layer(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full.astype(np.float32) / 255\n",
    "X_test = X_test.astype(np.float32) / 255\n",
    "\n",
    "X_val, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_val, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Iteration 1718/1718 - loss: 0.5052 - sparse_categorical_accuracy: 0.8195 - val_loss: 0.3969 - val_accuracy: 0.8608\n",
      "Epoch 2/5\n",
      "Iteration 1718/1718 - loss: 0.4131 - sparse_categorical_accuracy: 0.8530 - val_loss: 0.4044 - val_accuracy: 0.8562\n",
      "Epoch 3/5\n",
      "Iteration 1718/1718 - loss: 0.3808 - sparse_categorical_accuracy: 0.8627 - val_loss: 0.4585 - val_accuracy: 0.8536\n",
      "Epoch 4/5\n",
      "Iteration 1718/1718 - loss: 0.3702 - sparse_categorical_accuracy: 0.8639 - val_loss: 0.3827 - val_accuracy: 0.8658\n",
      "Epoch 5/5\n",
      "Iteration 1718/1718 - loss: 0.3599 - sparse_categorical_accuracy: 0.8702 - val_loss: 0.3920 - val_accuracy: 0.8642\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    \n",
    "    for step in range(1, n_steps + 1):\n",
    "        batch_idx = np.random.randint(len(X_train), size=batch_size)\n",
    "        X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "            \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "            \n",
    "        status = OrderedDict()\n",
    "        mean_loss(loss)\n",
    "        status['loss'] = mean_loss.result().numpy()\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "            status[metric.name] = metric.result().numpy()\n",
    "            \n",
    "        print(f\"\\rIteration {step}/{n_steps} - loss: {status['loss']:.4f} - {metric.name}: {status[metric.name]:.4f}\", end='')\n",
    "            \n",
    "    y_pred = model(X_val)\n",
    "    status['val_loss'] = np.mean(loss_fn(y_val, y_pred))\n",
    "    status['val_accuracy'] = np.mean(keras.metrics.sparse_categorical_accuracy(\n",
    "        tf.constant(y_val, dtype=np.float32), y_pred))\n",
    "    \n",
    "    print(f\" - val_loss: {status['val_loss']:.4f} - val_accuracy: {status['val_accuracy']:.4f}\")\n",
    "    \n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "lower_layers = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "])\n",
    "upper_layers = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model = keras.models.Sequential([\n",
    "    lower_layers, upper_layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_optimizer = keras.optimizers.SGD(lr=1e-4)\n",
    "upper_optimizer = keras.optimizers.Nadam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Iteration 1718/1718 - loss: 1.0429 - sparse_categorical_accuracy: 0.6892 - val_loss: 0.6890 - val_accuracy: 0.7810\n",
      "Epoch 2/5\n",
      "Iteration 1718/1718 - loss: 0.6449 - sparse_categorical_accuracy: 0.7821 - val_loss: 0.5885 - val_accuracy: 0.8020\n",
      "Epoch 3/5\n",
      "Iteration 1718/1718 - loss: 0.5771 - sparse_categorical_accuracy: 0.8008 - val_loss: 0.5477 - val_accuracy: 0.8124\n",
      "Epoch 4/5\n",
      "Iteration 1718/1718 - loss: 0.5441 - sparse_categorical_accuracy: 0.8104 - val_loss: 0.5253 - val_accuracy: 0.8128\n",
      "Epoch 5/5\n",
      "Iteration 1718/1718 - loss: 0.5285 - sparse_categorical_accuracy: 0.8143 - val_loss: 0.5140 - val_accuracy: 0.8218\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{n_epochs}\")\n",
    "    \n",
    "    for step in range(1, n_steps + 1):\n",
    "        batch_idx = np.random.randint(len(X_train), size=batch_size)\n",
    "        X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "            \n",
    "        for layers, optimizer in ((lower_layers, lower_optimizer), (upper_layers, upper_optimizer)):\n",
    "            gradients = tape.gradient(loss, layers.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
    "        del tape\n",
    "        \n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "            \n",
    "        status = OrderedDict()\n",
    "        mean_loss(loss)\n",
    "        status['loss'] = mean_loss.result().numpy()\n",
    "        \n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "            status[metric.name] = metric.result().numpy()\n",
    "            \n",
    "        print(f\"\\rIteration {step}/{n_steps} - loss: {status['loss']:.4f} - {metric.name}: {status[metric.name]:.4f}\", end='')\n",
    "            \n",
    "    y_pred = model(X_val)\n",
    "    status['val_loss'] = np.mean(loss_fn(y_val, y_pred))\n",
    "    status['val_accuracy'] = np.mean(keras.metrics.sparse_categorical_accuracy(\n",
    "        tf.constant(y_val, dtype=np.float32), y_pred))\n",
    "    \n",
    "    print(f\" - val_loss: {status['val_loss']:.4f} - val_accuracy: {status['val_accuracy']:.4f}\")\n",
    "    \n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:handson-tf2]",
   "language": "python",
   "name": "conda-env-handson-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
